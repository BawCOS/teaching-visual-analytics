[
["index.html", "Lesson Modules for Teaching Visual Analytics Preface", " Lesson Modules for Teaching Visual Analytics David Ebert Bradley Warner 2018-07-05 Preface "],
["who-is-this-book-for.html", "0.1 Who is this book for?", " 0.1 Who is this book for? This book is intended to provide the reader an introduction to visual analytics and notes to teach topics on visual analytics. Visual analytics is simply a cycle of analytic reasoning, visualization, and interaction. The book is primarily designed to provide structures and ideas on how to teach others about visual analytics. It provides two examples, the first has a heavy emphasis on analytic reasoning. The second has a heavy emphasis on exploration using visualization and interaction. An instructor can use all or portions of each chapter as necessary to meet the desired educational outcomes. "],
["book-structure-and-how-to-use-it.html", "0.2 Book Structure and How to Use It", " 0.2 Book Structure and How to Use It 0.2.1 Introduction This is a work is the result of an effort between the Visual Analytics for Command, Control, and Interoperability Environments (VACCINE) Center at Purdue University and the United States Air Force Academy to develop a smaller set of lessons to introduce the field of visual analytics (VA). Since many institutions do not have the resources to offer complete courses on VA, it was decided that standalone lessons plans have the potential for more use and dissemination. 0.2.2 Intent These modules are designed to help participants: Understand the definition and value of visual analytics Explain and use design principles for visual analytics Use visual analytics to explore and reach a decision for a problem 0.2.3 Use The first chapter introduces visual analytics through a simple small example that emphasizes analytic reasoning. It starts with a question and proceeds through the visual analytic cycle in an attempt to answer the question. In the process, the reader/instructor can different aspects of the cycle. which can be understood to be the analytic reasoning leg of the visual analytics cycle. The second chapter is also an introduction to visual analytics but it takes more of a hypothesis generating or exploration point of view and thus has more emphasis on visualization and interaction. Each lesson can be modified to change the length. They are written to comprise around 3 one hour lessons. However, and instructor can alter the length either way by increasing emphasis on certain points or dropping some topics. Each lesson is comprised of a sequence of ideas that leads students through several iterations of the visual analytics cycle. Each idea is supported with code and suggested discussion ideas. "],
["prerequisites.html", "0.3 Prerequisites", " 0.3 Prerequisites There only a minimal assumed skills and knowledge for these lessons. The students should have some basic understanding of statistics and the ability to use basic algebra skills. Since visual analytics is heavily dependent on visualization and interaction it is heavily dependent on software. There are a number of choices that could be used for software; all of them have advantages and disadvantages. We choice to use R because it is a commonly used software, supports many different packages, and can be used to generate an interactive html document. There are no assumed knowledge or computing skills for these lessons. The instructor can have the participants write code or use the output of the code provided in these notes. There are of course some limitations to using html rendered results but these are generally minor in nature. Code for most of the examples has been provided for most of the examples to help the instructor recreate and modify the results. "],
["software-packages.html", "0.4 Software Packages", " 0.4 Software Packages These notes make use of the following packages in R knitr (Xie 2018c), rmarkdown (Allaire et al. 2017), Hmisc (Harrell 2018), lattice (Sarkar 2017), vcd (Meyer, Zeileis, and Hornik 2017), ggplot2 (Wickham and Chang 2016), MASS (Ripley 2018), TeachingDemos (Snow 2016), Stat2Data (Lock 2013), car (Fox, Weisberg, and Price 2018), DT (Xie 2018b),crosstalk (Cheng 2016), leaflet (Cheng, Karambelkar, and Xie 2017), plotly (Sievert et al. 2017), ggrepel(Slowikowski 2018), rpivotTable(Martoglio 2017), RColorBrewer(Neuwirth 2014), gapminder(Bryan 2017), tidyverse(Wickham 2017), readxl(Wickham and Bryan 2017), kableExtra(Zhu 2018), GGally(Schloerke et al. 2018), ggthemes(Arnold 2018), rbokeh(Hafen and Continuum Analytics, Inc. 2016), trelliscopejs(Hafen and Schloerke 2018), DT(Xie 2018b), geofacet(Hafen 2018). References "],
["acknowledgements.html", "0.5 Acknowledgements", " 0.5 Acknowledgements This work has been supported with a grant from the Department of Homeland Security. Many ideas were generated from a workshop conducted at the United States Air Force Academy and included faculty from Georgia Tech, Purdue, Unites States Air Force Academy, and Prarie View A&amp;M. Key contributors , GT, UCGA, Purdue, USAFA …. This book was written using the excellent bookdown package (Xie 2018a) in R. The book was run on Windows machine using RStudio. The following information gives the software and package versions: devtools::session_info(c(&quot;tidyverse&quot;)) ## Session info ------------------------------------------------------------- ## setting value ## version R version 3.4.3 (2017-11-30) ## system x86_64, mingw32 ## ui RTerm ## language (EN) ## collate English_United States.1252 ## tz America/Denver ## date 2018-07-05 ## Packages ----------------------------------------------------------------- ## package * version date source ## assertthat 0.2.0 2017-04-11 CRAN (R 3.4.3) ## backports 1.1.2 2017-12-13 CRAN (R 3.4.3) ## base64enc 0.1-3 2015-07-28 CRAN (R 3.3.0) ## BH 1.66.0-1 2018-02-13 CRAN (R 3.4.3) ## bindr 0.1.1 2018-03-13 CRAN (R 3.4.4) ## bindrcpp 0.2.2 2018-03-29 CRAN (R 3.4.4) ## broom 0.4.4 2018-03-29 CRAN (R 3.4.4) ## callr 2.0.4 2018-05-15 CRAN (R 3.4.4) ## cellranger 1.1.0 2016-07-27 CRAN (R 3.4.3) ## cli 1.0.0 2017-11-05 CRAN (R 3.4.3) ## colorspace 1.3-2 2016-12-14 CRAN (R 3.3.2) ## compiler 3.4.3 2017-12-06 local ## crayon 1.3.4 2017-09-16 CRAN (R 3.4.3) ## curl 3.1 2017-12-12 CRAN (R 3.4.3) ## DBI 1.0.0 2018-05-02 CRAN (R 3.4.4) ## dbplyr 1.2.1 2018-02-19 CRAN (R 3.4.3) ## dichromat 2.0-0 2013-01-24 CRAN (R 3.3.0) ## digest 0.6.15 2018-01-28 CRAN (R 3.4.3) ## dplyr 0.7.5 2018-05-19 CRAN (R 3.4.4) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.4.3) ## forcats 0.3.0 2018-02-19 CRAN (R 3.4.4) ## foreign 0.8-70 2018-04-23 CRAN (R 3.4.4) ## ggplot2 2.2.1 2016-12-30 CRAN (R 3.3.2) ## glue 1.2.0 2017-10-29 CRAN (R 3.4.3) ## graphics * 3.4.3 2017-12-06 local ## grDevices * 3.4.3 2017-12-06 local ## grid 3.4.3 2017-12-06 local ## gtable 0.2.0 2016-02-26 CRAN (R 3.3.0) ## haven 1.1.0 2017-07-09 CRAN (R 3.4.3) ## highr 0.6 2016-05-09 CRAN (R 3.3.0) ## hms 0.4.0 2017-11-23 CRAN (R 3.4.3) ## htmltools 0.3.6 2017-04-28 CRAN (R 3.4.3) ## httr 1.3.1 2017-08-20 CRAN (R 3.4.3) ## jsonlite 1.5 2017-06-01 CRAN (R 3.3.3) ## knitr 1.20 2018-02-20 CRAN (R 3.4.4) ## labeling 0.3 2014-08-23 CRAN (R 3.3.0) ## lattice 0.20-35 2017-03-25 CRAN (R 3.4.3) ## lazyeval 0.2.1.9000 2018-01-26 Github (hadley/lazyeval@93c455c) ## lubridate 1.7.2 2018-02-06 CRAN (R 3.4.3) ## magrittr 1.5 2014-11-22 CRAN (R 3.3.1) ## markdown 0.8 2017-04-20 CRAN (R 3.4.3) ## MASS 7.3-50 2018-04-30 CRAN (R 3.4.4) ## methods * 3.4.3 2017-12-06 local ## mime 0.5 2016-07-07 CRAN (R 3.3.1) ## mnormt 1.5-5 2016-10-15 CRAN (R 3.3.2) ## modelr 0.1.1 2017-07-24 CRAN (R 3.4.3) ## munsell 0.4.3 2016-02-13 CRAN (R 3.3.0) ## nlme 3.1-131 2017-02-06 CRAN (R 3.3.2) ## openssl 0.9.9 2017-11-10 CRAN (R 3.4.3) ## parallel 3.4.3 2017-12-06 local ## pillar 1.2.3 2018-05-25 CRAN (R 3.4.4) ## pkgconfig 2.0.1 2017-03-21 CRAN (R 3.4.3) ## plogr 0.2.0 2018-03-25 CRAN (R 3.4.4) ## plyr 1.8.4 2016-06-08 CRAN (R 3.3.0) ## praise 1.0.0 2015-08-11 CRAN (R 3.3.0) ## processx 3.1.0 2018-05-15 CRAN (R 3.4.4) ## psych 1.7.8 2017-09-09 CRAN (R 3.4.3) ## purrr 0.2.4 2017-10-18 CRAN (R 3.4.4) ## R6 2.2.2 2017-06-17 CRAN (R 3.3.3) ## RColorBrewer 1.1-2 2014-12-07 CRAN (R 3.3.0) ## Rcpp 0.12.17 2018-05-18 CRAN (R 3.4.4) ## readr 1.1.1 2017-05-16 CRAN (R 3.4.3) ## readxl 1.0.0 2017-04-18 CRAN (R 3.4.3) ## rematch 1.0.1 2016-04-21 CRAN (R 3.4.3) ## reprex 0.1.2 2018-01-26 CRAN (R 3.4.3) ## reshape2 1.4.3 2017-12-11 CRAN (R 3.4.3) ## rlang 0.2.0 2018-02-20 CRAN (R 3.4.3) ## rmarkdown 1.8 2017-11-17 CRAN (R 3.4.3) ## rprojroot 1.3-2 2018-01-03 CRAN (R 3.4.3) ## rstudioapi 0.7 2017-09-07 CRAN (R 3.4.3) ## rvest 0.3.2 2016-06-17 CRAN (R 3.3.3) ## scales 0.5.0 2017-08-24 CRAN (R 3.4.3) ## selectr 0.3-1 2016-12-19 CRAN (R 3.3.3) ## stats * 3.4.3 2017-12-06 local ## stringi 1.1.7 2018-03-12 CRAN (R 3.4.4) ## stringr 1.3.1 2018-05-10 CRAN (R 3.4.4) ## testthat 2.0.0 2017-12-13 CRAN (R 3.4.3) ## tibble 1.4.2 2018-01-22 CRAN (R 3.4.4) ## tidyr 0.8.1 2018-05-18 CRAN (R 3.4.4) ## tidyselect 0.2.4 2018-02-26 CRAN (R 3.4.4) ## tidyverse 1.2.1 2017-11-14 CRAN (R 3.4.3) ## tools 3.4.3 2017-12-06 local ## utf8 1.1.3 2018-01-03 CRAN (R 3.4.3) ## utils * 3.4.3 2017-12-06 local ## viridisLite 0.3.0 2018-02-01 CRAN (R 3.4.4) ## whisker 0.3-2 2013-04-28 CRAN (R 3.0.2) ## withr 2.1.2 2018-03-15 CRAN (R 3.4.4) ## xml2 1.1.1 2017-01-24 CRAN (R 3.3.2) ## yaml 2.1.16 2017-12-12 CRAN (R 3.4.3) This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],
["Chpt1.html", "Chapter 1 Introducing Visual Analytics - Emphasis on Analytic Reasoning", " Chapter 1 Introducing Visual Analytics - Emphasis on Analytic Reasoning “The purpose of computing is insight, not numbers.” – Richard Hamming “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey This chapter is an introduction to visual analytics. It starts with a question, which can be understood to be the analytic reasoning leg of the visual analytics cycle. The second chapter is also an introduction to visual analytics but it takes more of a hypothesis generating point of view and thus has more emphasis on visualization and interaction. The depth that you as the instructor use depends on how much time can be devoted to this lesson. This lesson could be done in a little as 30 minutes or as much as three 50 minute lessons. These notes are broken into sections. Under each section are some combination of definitions, explanations, discussion ideas, and problems as needed. You can drop portions or entire sections as needed or dictated by time. Many of the ideas for this chapter come from friendly’s excellent book on the analysis of discrete data (Friendly and Meyer 2016). References "],
["lesson-preparation.html", "1.1 Lesson Preparation", " 1.1 Lesson Preparation 1.1.1 Objectives After the training, participants will: be able to define and explain visual analytics be able to describe the three important components of visual analytics be able to discuss types of unique insights that visual analytics can provide 1.1.2 Preparation Prior to class: Watch the Beauty of Data Visualization - David McCandless Watch TEDxWaterloo - Miriah Meyer - Information Visualization for Scientific Discovery Read the article Visual Analytics: Definition, Process, and Challenges by Daniel Keim, Gennady Andrienko, Jean-Daniel Fekete, Carsten Gorg,Jorn Kohlhammer, and Guy Melan "],
["Intro1.html", "1.2 Lesson Introduction", " 1.2 Lesson Introduction Suggestions to start the lesson: Have the participants discuss the videos and explain insights gained for the preparation work. One of the objectives is to define and explain visual analytics. A useful definition is (this can be given or developed by asking students for input): Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces. Discussion ideas and points for this portion of the lesson include: The goal of visual analytics is to create software systems that will support the analytical reasoning process. Visual analytics can be broken down into a form of analytical reasoning that relies on visual displays that are interactive. We must in turn explore and understand analytical reasoning, visual representation of data, and interactive interfaces. This is a lead into the rest of this lesson. Below is a diagram of the visual analytics process. It is a cycle where any one particular step can be a starting point, and ending point, and a transition point to another area of the cycle. For this lesson we will assume you start with a hypothesis and thus start with analytical reasoning. It is important to note that this is not always the case. You could start with data visualization in an exploratory manner and then move to either analytic reasoning or data visualization. Figure 1.1: Visual Analytics Cycle "],
["AR1.html", "1.3 Analytic Reasoning", " 1.3 Analytic Reasoning This section introduces the concept of analytic reasoning and then uses an example to illustrate the ideas. 1.3.1 Analytic Reasoning Explained The goal of analysis is to make a decision about a question. This question can be part of a larger analysis problem with many questions. Analytic reasoning requires reasoning logically with the support of numeric evidence. The analyst must understand the limitations of the data and the assumptions of the data transformations and summaries. Again it is important to note that analytics reasoning works in conjunction, not separate from, visualization and interaction. In other words, visual analytics takes analytical reasoning further by taking advantages of a visual and interactive representation of data to aid in the creation of knowledge products or actions. Many analytical tasks follow a process of • Information gathering • Re-representation of the information in a form that aids analysis • Development of insight through the manipulation of this representation • Creation of some knowledge product or direct action based on the knowledge insight. 1.3.2 Example We will introduce an example to highlight the ideas and principles of analytic reasoning. The example we selected is easy to understand and allows us to present some of the important concepts of visual analytics. You can pick an example of your own, but remember to keep it simple and easy to understand. 1.3.2.1 Problem: Often in baseball, teams and fans want to compare players. In the 1990s two extraordinary players were Derek Jeter of the New York Yankees and David Justice of the Atlanta Braves. 1.3.2.2 Question: Who was the better baseball player? Take some time and discuss how to answer the question. 1.3.2.3 Discussion ideas: There are many ways to try to answer this question. Someone might say that Jeter was paid more therefore he is better. Or someone would say that the Yankees are a better team and thus Jeter is better. This is known as butterfly reasoning. Typically students will not use an analytic reasoning process but more of an intuition or perhaps just a bias. We have forced the students into an intuitive answer which will likely be based on biases and not empirical evidence. Next we will guide the students further along the path of analytic reasoning by narrowing the scope of the problem and introducing data. 1.3.3 Second Example As another example, but this time using a social context, is the University of California - Berkeley admission bias question. Running this as a lesson would look like this: 1.3.3.1 Problem: In the early 1970s there was a concern that the University of California - Berkeley was discriminating in its admissions process based on gender. Question: How do we decide if Cal-Berkeley is practicing discrimination based on gender? Take some time and discuss how to answer the question. 1.3.3.2 Discussion ideas: There are many ways to try to answer this question. Someone might say that Berkeley had to be because this was the early 1970s and we know that institutional bias against women was common. This is known as butterfly reasoning. This is not analytical reasoning. We must collect some data that will help us in deciding the answer. In practice the is an iterative process. 1.3.4 Other problem ideas: In mathematics, deciding if a function is continuous at a point. Generate a visual of a function and have an epsilon selected at random. The decision maker must determine if there is a delta such that for all the points inside delta of x, the function is within epsilon of the limit. In political science, we could use voter data to ask questions about coastal states versus central states. In biology, we could look at sports data such as Olympic race times. In engineering, we can look at strength of materials, completion times of projects, or design selection. 1.3.5 Using Analytic Reasoning. We need data and a metric to help us answer the question of who is the better batter. There are many different ways to gather the data but this should be based on the metric we use. In sports analytics, there are many metrics that can be used to compare players, but for ease and the sake of an interesting analysis, we will use batting average. To make the problem even easier, we will provide the raw data from the years 1995 and 1996. Some students may object to this simplified data and you can use these objections as points of discussion on the importance of data that represents the question being asked. 1.3.5.1 Information Gathering Here is the code to generate the raw data for both batters. The reason we are generating the data is that we simply have the summary data from the records and this is not how data is often obtained. The information gathering and cleaning can often represent a significant time investment. This is often lost on students as data is presented to them in a clean format because of class time concerns. To give the students a sense of the work involved, generate the raw data using the code below, and then give the csv file to them. First, we will generate a simple frequency form of the data, we are actually working backwards from how you would typically proceed in your analysis but this is done to save you time as an instructor. For this section we will need several packages; let’s load them first. library(vcd) library(vcdExtra) library(dplyr) MLB_freq&lt;-data.frame(expand.grid(Player=c(&quot;Derek Jeter&quot;,&quot;David Justice&quot;), Year=c(&quot;1995&quot;,&quot;1996&quot;),Result=c(&quot;Hit&quot;,&quot;Out&quot;)), count=c(12,104,183,45,48-12,411-104,582-183,140-45)) knitr::kable(MLB_freq) Player Year Result count Derek Jeter 1995 Hit 12 David Justice 1995 Hit 104 Derek Jeter 1996 Hit 183 David Justice 1996 Hit 45 Derek Jeter 1995 Out 36 David Justice 1995 Out 307 Derek Jeter 1996 Out 399 David Justice 1996 Out 95 MLB_long&lt;-expand.dft(MLB_freq,freq=&quot;count&quot;) head(MLB_long) ## Player Year Result ## 1 Derek Jeter 1995 Hit ## 2 Derek Jeter 1995 Hit ## 3 Derek Jeter 1995 Hit ## 4 Derek Jeter 1995 Hit ## 5 Derek Jeter 1995 Hit ## 6 Derek Jeter 1995 Hit Now let’s random shuffle to give the appearance of collected data. Note: we are using the dplyr package to complete this work. table(MLB_long$Year) ## ## 1995 1996 ## 459 722 set.seed(81) MLB_long&lt;-MLB_long %&gt;% sample_frac() %&gt;% arrange(Year) ## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 head(MLB_long) ## Player Year Result ## 1 Derek Jeter 1995 Hit ## 2 David Justice 1995 Out ## 3 David Justice 1995 Out ## 4 David Justice 1995 Hit ## 5 David Justice 1995 Out ## 6 David Justice 1995 Out table(MLB_long) ## , , Result = Hit ## ## Year ## Player 1995 1996 ## David Justice 104 45 ## Derek Jeter 12 183 ## ## , , Result = Out ## ## Year ## Player 1995 1996 ## David Justice 307 95 ## Derek Jeter 36 399 Now to get the data to students you could use the write.csv function write.csv(MLB_long,&quot;./data/batting.csv&quot;) 1.3.5.2 Transformation (Re-representation) of Data The data is not in a form that will help make a decision. It is important to ask the students how they would transform the data to help make a decision. There are typically two responses to this request. The first is to get the batting averages for the two players. The second is to summarize the batting averages by each player within each year. This then begs the question of how to aggregate the two years. At this point, try to steer the students to comparing the batting averages within each year. They will note that David Justice has the higher in each year. The next question is how to transform this information into a single metric of overall batting averages. Also note at this point that our transformation is in the form of descriptive numeric summaries. This is often an important first step but in the next section we will work with visual representations of the data. Now, let’s first get the batting average for each year. 1.3.5.3 Problem How should we transform, summarize, the data in a manner that will help us make a decision? 1.3.5.3.1 Discussion Ideas There are many ways to summarize the data. The problem is that we have hits and outs whereas we need total at bats. If we create summary, we will not get an appropriate metric of performance. summary(MLB_long) ## Player Year Result ## David Justice:551 Min. :1995 Hit:344 ## Derek Jeter :630 1st Qu.:1995 Out:837 ## Median :1996 ## Mean :1996 ## 3rd Qu.:1996 ## Max. :1996 We are going have to transform the data. The first step would be to group the data by each of the variables. MLB_long %&gt;% dplyr::group_by(Year,Player,Result) %&gt;% dplyr::summarize(Total=n()) ## # A tibble: 8 x 4 ## # Groups: Year, Player [?] ## Year Player Result Total ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 1995 David Justice Hit 104 ## 2 1995 David Justice Out 307 ## 3 1995 Derek Jeter Hit 12 ## 4 1995 Derek Jeter Out 36 ## 5 1996 David Justice Hit 45 ## 6 1996 David Justice Out 95 ## 7 1996 Derek Jeter Hit 183 ## 8 1996 Derek Jeter Out 399 Or using different code knitr::kable(xtabs(~Player+Result+Year,MLB_long),caption = &quot;A Simple Table Summarizing Batting Performance&quot;) Table 1.1: A Simple Table Summarizing Batting Performance Player Result Year Freq David Justice Hit 1995 104 Derek Jeter Hit 1995 12 David Justice Out 1995 307 Derek Jeter Out 1995 36 David Justice Hit 1996 45 Derek Jeter Hit 1996 183 David Justice Out 1996 95 Derek Jeter Out 1996 399 This is a good first step but students will have to mentally try to find the total at bats, sum hits and outs, and then divide hits by at bats; see Table 1.1 and the simple table below it. As a preview of the visualization section, consider the following table and think about how it presents the information. ftable(xtabs(~Player+Year+Result,MLB_long)) ## Result Hit Out ## Player Year ## David Justice 1995 104 307 ## 1996 45 95 ## Derek Jeter 1995 12 36 ## 1996 183 399 It should be clear that it is hard to use these summaries to make a decision. In the next section we will discuss visualizing data to include the advantages and disadvantages of each type of table. But for now let’s complete the discussion of comparing batting averages. Next we need to get a batting average for each player in each year. prop.table(xtabs(~Player+Year+Result,MLB_long),1:2)[,,1] ## Year ## Player 1995 1996 ## David Justice 0.2530414 0.3214286 ## Derek Jeter 0.2500000 0.3144330 or as in Table 1.2. knitr::kable(MLB_long %&gt;% group_by(Player,Year,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player,Year) %&gt;% summarise(At_Bats=n()),by=c(&quot;Player&quot;,&quot;Year&quot;)) %&gt;% select(-Result) %&gt;% mutate(Avg=round(Hits/At_Bats,4)) %&gt;% select(Player,Year,Avg) %&gt;% arrange(Year,desc(Player)),caption=&quot;Batting Average in each Year&quot;) Table 1.2: Batting Average in each Year Player Year Avg Derek Jeter 1995 0.2500 David Justice 1995 0.2530 Derek Jeter 1996 0.3144 David Justice 1996 0.3214 1.3.5.4 Discussion Ideas Ask the students to decide who is the better player based on the batting averages. The students should point out that David Justice had a better batting average in each year. You can ask how you should combine these averages? The students may claim that it does not matter since David Justice was better in each year. They may also want to take a simple average, which again would lead to David Justice being the better player. Other students will note that the overall batting average is found by dividing totals hits by total at bats. Let’s combine the hits and outs from each year by aggregating across years. prop.table(xtabs(~Player+Result,MLB_long),1)[,1] ## David Justice Derek Jeter ## 0.2704174 0.3095238 knitr::kable(MLB_long %&gt;% group_by(Player,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player) %&gt;% summarise(Total=n()),by=&quot;Player&quot;) %&gt;% select(-Result) %&gt;% mutate(Avg=round(Hits/Total,4)) %&gt;% select(Player,Avg) %&gt;% arrange(desc(Player)),caption=&quot;Combined Batting Average for Both Years&quot;) Table 1.3: Combined Batting Average for Both Years Player Avg Derek Jeter 0.3095 David Justice 0.2704 From Table 1.3 it now appears that Derek Jeter is the better player. Who is the better player? Why did we get different results depending on the summary? What concerns do you have with the summaries and data transformations we created? "],
["Viz1.html", "1.4 Visualizing data (Re-representation)", " 1.4 Visualizing data (Re-representation) We are still in the transformation or re-representation of information phase. Visualization helps in this phase of the analysis project. We will present many different types of visualizations and this is a transition to the next phase of the analysis titled manipulation. Having to write code over and over with small changes will lead us to the interaction phase. We will first address visualization. Although some authors would not consider tables as a visualization of data, we do. In this section, we will use two different forms of visualization, both a table and a vector plot. 1.4.1 Background on Visualizing Data Data visualization is a complicated and important subject. You can get advanced degrees in the subject. We will only give the basics principles and ideas in this section. There are three things that should always inform the visual summary you select and/or design. First, you must decide which ideas are important to communicate to the reader. This requires you to identify your key message, filter and summarize the data to a point that maintains that message without over-compressing the data, and finally think about how to communicate this idea. Second, what is the reader’s background and how will they understand or what will they take away from the visualization? Tufte states that you should “respect your audience”. Think about such factors as how short-term memory limits the amount of information a reader can hold and how preattentive thought quickly picks up position, motion, shape, and color first. Finally, consider the data type(s), what are the advantages and disadvantages of the data, what information is contained in the data, and what tools are needed to visualize this data? For example, color does a nice job with discrete variables although it could be used as a gradient for continuous variables. Length is good for continuous variables. Area is difficult for readers to compare although rectangular is not as difficult because of the linear comparison of each side. Figure 1.2, adopted from Friendly and Meyer (2016), is an excellent framework for thinking about visualization. Often, we first start in the analysis phase. For our current problem we are in the analysis phase with comparison being the primary design principle; we want to compare Derek Jeter to David Justice. At the end of the analysis we must present the results and this puts us in the presentation section. In each case the reader is different. In the analysis phase, we, the analyst, are the reader and maybe a few others on the analysis team. In the presentation, the readers can be from a variety of backgrounds often leading to a simpler presentation. Figure 1.2: Visualization Purposes 1.4.2 Using Problem In the batting average problem, we have compressed the results of individual bats into an average. There was also the choice to leave year as a separate variable or to compress further into a single summary. The compression to a batting average, weather accounting for year or not, lost an important element of the data and that is the magnitude of number of bats. Understanding our reader, in this case the analyst but ultimately any reader for presentation, we should expect that most people will consider the value of the batting averages for each player during each year to be of equal value, that is they assume that the number of at bats is approximately equal. Thus, in this example they will either compare Derek Jeter against David Justice each year or simply average the two years into a single number to make a decision on the better batter. In either case, the reader may conclude that David Justice is the better batter. In developing our visual summaries we will have to take this into consideration. 1.4.3 Discussion ideas The tables in the previous section are visual summaries but what are the limitations? What are the key idea(s) for this problem? How would you visualize this data? Most students will only think about tables. In considering tables, how should the variables be ordered? That is, which variables should be rows and which should be columns? Should you use shading in the tables? 1.4.4 Visualization We will examine two ways to visualize the data in our batting average problem. First we will continue to look at the tables and then move to a visualization that utilizes vectors. 1.4.4.1 Tables for Visualization In this section we will go through a series of tables and discuss advantages and disadvantages of each. 1.4.4.1.1 First Table The first table from the software is a default and looks like table(MLB_long) ## , , Result = Hit ## ## Year ## Player 1995 1996 ## David Justice 104 45 ## Derek Jeter 12 183 ## ## , , Result = Out ## ## Year ## Player 1995 1996 ## David Justice 307 95 ## Derek Jeter 36 399 Ask your students about their thoughts on this table. Some ideas that may come include the lack of a title, the use of “, ,”, and having the split on result because it is difficult to mentally combine for comparison. Remember that the reader of this table is the analytic team. As an analyst, the use of “, ,”&quot; in the titles and the splitting of result into two table may not bother you but it is still inefficient. Plus you as the analyst want the ability to control how the variables are arranged. 1.4.4.1.2 Additional Tables Let’s start by finding a better representation of the table. ftable(table(MLB_long)) ## Result Hit Out ## Player Year ## David Justice 1995 104 307 ## 1996 45 95 ## Derek Jeter 1995 12 36 ## 1996 183 399 The advantage of this figure did not split the data. Note that the software automatically placed Player and Year in the rows. This placement makes it difficult to compare the years since they are separated by one line. Next, we will sort the variables by the comparison we want to make. In this case we want players within years. This is called effect-ordering sorting (Friendly and Kwan 2003). ftable(table(MLB_long),row.vars=c(&quot;Year&quot;,&quot;Player&quot;)) ## Result Hit Out ## Year Player ## 1995 David Justice 104 307 ## Derek Jeter 12 36 ## 1996 David Justice 45 95 ## Derek Jeter 183 399 As we discussed in Section 1.2, the transitions between analysis, visualization, and interaction may be a cycle. So next we need to get the number of at bats. data.frame(MLB_long %&gt;% group_by(Player,Year,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player,Year) %&gt;% summarise(At_Bats=n()),by=c(&quot;Player&quot;,&quot;Year&quot;)) %&gt;% select(-Result) %&gt;% arrange(Year)) ## Player Year Hits At_Bats ## 1 David Justice 1995 104 411 ## 2 Derek Jeter 1995 12 48 ## 3 David Justice 1996 45 140 ## 4 Derek Jeter 1996 183 582 This is getting better although we lost year as an outer row variable and we still need average instead of hits. data.frame(MLB_long %&gt;% group_by(Player,Year,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player,Year) %&gt;% summarise(At_Bats=n()),by=c(&quot;Player&quot;,&quot;Year&quot;)) %&gt;% mutate(Average=Hits/At_Bats) %&gt;% select(Year, Player,Average,At_Bats) %&gt;% arrange(Year)) ## Year Player Average At_Bats ## 1 1995 David Justice 0.2530414 411 ## 2 1995 Derek Jeter 0.2500000 48 ## 3 1996 David Justice 0.3214286 140 ## 4 1996 Derek Jeter 0.3144330 582 This is a nice summary and makes it clear that we have years with a small number of at bats. We still have to clean up some things to improve the visualization. What are some ideas to improve it? A caption should be provided with enough detail to describe the table. In addition, the default of 7 decimal places is too much. We will use 4 decimal places. knitr::kable(data.frame(MLB_long %&gt;% group_by(Player,Year,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player,Year) %&gt;% summarise(At_Bats=n()),by=c(&quot;Player&quot;,&quot;Year&quot;)) %&gt;% mutate(Average=round(Hits/At_Bats,4)) %&gt;% select(Year, Player,Average,At_Bats) %&gt;% arrange(Year)),caption=&quot;Batting Averages and Total at Bats for Both Years&quot;) Table 1.4: Batting Averages and Total at Bats for Both Years Year Player Average At_Bats 1995 David Justice 0.2530 411 1995 Derek Jeter 0.2500 48 1996 David Justice 0.3214 140 1996 Derek Jeter 0.3144 582 Finally, we should combine both years. It is clear that we can’t just average the averages since the total at bats is different. We need to find the total hits and divide by the total at bats. This is Table 1.3 with the addition of the total at bats. knitr::kable(MLB_long %&gt;% group_by(Player,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player) %&gt;% summarise(At_Bats=n()),by=&quot;Player&quot;) %&gt;% select(-Result) %&gt;% mutate(Average=round(Hits/At_Bats,4)) %&gt;% select(Player,Average,At_Bats) %&gt;% arrange(desc(Player)),caption=&quot;Combined Batting Average for Both Years with Total at Bats&quot;) Table 1.5: Combined Batting Average for Both Years with Total at Bats Player Average At_Bats Derek Jeter 0.3095 630 David Justice 0.2704 551 The two players had similar number of at bats but Derek Jeter had the higher batting average. 1.4.5 Vectors for Visualization In the previous section, we learned that representing performance with just batting average is potentially misleading. The decimal representation of the average is too much of a compression because we lose the important information on the number of at bats. Carried to an extreme, if a player only batted twice and got one hit, batting average would be reported as a staggering 0.500. We attempted to remedy this by also reporting the total number of at bats along side of the batting average. The other issue we had was in the bias people use to combine batting averages. If only batting averages are reported, people tend to want to average them. However, since the number of at bats changes from year to year a weighted average is needed. In fact the way to determine overall average is to add the total number of hits and divide by the total number of at bats. If in the first year is \\({a \\over b}\\) and the second year is \\({c \\over d}\\), then the overall average is \\({(a+c) \\over (b+d)}\\). This is vector addition and gives another way to visualize the data; in the form of vectors and vector addition. 1.4.5.1 Discussion Ideas Now that we have described batting average, how to combine years, and the importance of the total at bats, is there a way to represent these in a plot? 1.4.5.2 Plot To plot both batting average and at bats is difficult so we will plot at bats on the x-axis and hits on the y-axis. In this case, the batting average is the angle, or the slope of the line. Angles are difficult to estimate but easy to compare. We used color to separate the two batters only for the combined years. We included the individual year breakdown but they are difficult to see. We included points to bring them out. The combination of years, or the calculation of combined batting average, is a vector addition and not the addition of fractions. Vectors have both direction and length. The direction is the number of hits over number of at bats, the traditional batting average. The length is related to the number of at bats, it is really the Pythagorean length which combines hits and total at bats. The plot shows that Derek Jeter’s combined batting average gets pulled more towards the second year where he had more at bats. The opposite is true for David Justice and that is why we get the reversal of batting averages when we compress from two years down to one. Figure 1.3: Visualization of Batting Averages This might be hard to see in the document so you can open it separately as pdf to see the design of the graph better. The link is http://bit.ly/2tP05tU 1.4.5.3 Discussion Ideas What do you like about the plot? What are the strengths and weaknesses? What changes would you make? Is it better to put the players’ years end to end or does that hide that David Justice is better in each individual year? Does the color help or hurt? The angle is the batting average, should we use white space in he graph and put an explanation into the graph? What if we had three years, how would we represent them? The labels overlap the lines, should we move or offset them? As a lead into the next lesson on interaction, selecting a year and turning it off as well as making lines more transparent could help. The problems with this graph are that the lines are close together, the font size on the axis is too small, the label points are hidden by the lines, and the aspect ratio is not one so that it appears that the average is closer to a 45 degree line which is an average of 1.000. 1.4.5.4 Plots in R You can construct these plots in R although they are a bit awkward. However on the positive side we can control the aspect ratio, offset text, and change the font size. To get an idea of how to do this consider the simple plot in Figure 1.4. plot(1,type=&quot;n&quot;,xlim=c(0,800),ylim=c(0,800),xlab=&quot;At Bats&quot;,ylab=&quot;Hits&quot;,asp=1) arrows(0,0,200,50,col=&quot;red&quot;) arrows(200,50,400,150,col=&quot;red&quot;) arrows(0,0,400,150,col=&quot;red&quot;) abline(0,1) Figure 1.4: Simple Example of visualization of Batting Averages Now using our data, Figure 1.5 shows that Derek Jeter is being pulled up to his 1996 average and David Justice is being pulled down to his 1995 average. plot(1,type=&quot;n&quot;,xlim=c(0,700),ylim=c(0,200),xlab=&quot;At Bats&quot;,ylab=&quot;Hits&quot;,asp=1) arrows(0,0,48,12,col=&quot;black&quot;) arrows(48,12,(48+582),(183+12),col=&quot;black&quot;) arrows(0,0,(48+582),(183+12),col=&quot;red&quot;) arrows(0,0,411,104,col=&quot;black&quot;) arrows(411,104,(411+140),(104+45),col=&quot;black&quot;) arrows(0,0,(411+140),(104+45),col=&quot;blue&quot;) abline(0,1) title(&quot;Comparing Batting Averages&quot;) legend(1, 250, legend=c(&quot;Derek Jeter&quot;, &quot;David Justice&quot;), col=c(&quot;red&quot;, &quot;blue&quot;),lty=c(1,1), cex=0.8) Figure 1.5: Comparing Batting Averages for Derek Jeter and David Justice We can improve the look of the plot by using the ggplot2 (Wickham and Chang 2016) package. First let’s get the data in a form to better using the plotting function. plotdata&lt;-data.frame(MLB_long %&gt;% group_by(Player,Year,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player,Year) %&gt;% summarise(At_Bats=n()),by=c(&quot;Player&quot;,&quot;Year&quot;)) %&gt;% select(Year, Player,Hits,At_Bats) %&gt;% arrange(Year)) %&gt;% rbind(.,group_by(.,Player) %&gt;% summarise(Hits=sum(Hits),At_Bats=sum(At_Bats)) %&gt;% mutate(Year=&quot;Combined&quot;) %&gt;% select(Year, Player, Hits, At_Bats) ) Now Figure 1.6 compares the players. plotdata %&gt;% ggplot(aes(x=At_Bats,y=Hits)) + geom_point() + xlim(0,700)+ ylim(0,200)+ coord_fixed(ratio=1) + geom_segment(aes(x = 0, y = 0, xend = At_Bats, yend = Hits,color=Player),arrow=arrow(length=unit(.2,&#39;cm&#39;),type=&quot;closed&quot;)) + geom_text_repel(aes(x=At_Bats,y=Hits, label = Year)) + labs(title=&quot;Batting Averages&quot;) Figure 1.6: Comparing Batting Averages for Derek Jeter and David Justice (Using ggplot) 1.4.5.5 Discussion Ideas What do you like about Figure 1.6 as compared to Figure 1.5? What changes would you make? Is it better to put the players’ years end to end or does that hide that David Justice is better in each individual year? Does the color help or hurt? 1.4.5.6 More Discussion Ideas If you want to use external resources to help your students, the following are good references: http://junkcharts.typepad.com/junk_charts/ http://flowingdata.com/ http://www.storytellingwithdata.com/ References "],
["Inert1.html", "1.5 Interaction", " 1.5 Interaction The third phase of the analysis is to develop insight through manipulation of representations. In some sense we did this in section 1.4 by running different table and plots. This exercise was static and labor intensive. In this section we will introduce the use of interactive tools that reduce the need to re-write code. We will be using html widgets as this document is written in html. However, there are many other tools that can be used for interaction. 1.5.1 Background Information on Interaction The purpose of interaction in a visual display of data is the aid in the decision process. It can be used to explain a concept, develop conjectures or insights, and/or facilitate an exploration. This can lead to increased efficiencies and effectiveness of the decision making process. However, the science of understanding and implementing interaction is the least developed of the three areas. This is partly due to the only recent availability of software to easily develop customized interactive displays. The general principle of interaction is summarized as overview first, zoom on interest, filter to make specific, and provide details as required. Interaction can add in decision making because by interacting with the data; the analyst and decision maker can transform data, alter visual display, and then form hypothesis or alter conditions to determine impact. The biggest issue with interactivity is the ability to rapidly develop custom products. Currently this requires programming skills and sophisticated software. This will change, and has to some extent with packages such as Tableau, as more tools are developed that remove the need to program. 1.5.1.1 Simple Example To supplement this portion of the lesson, you may want to find interactive examples on the web. The following site has an example of a simple interactive visualization to help you facilitate a discussion on the effectiveness of an interactive to present an idea. “https://hbr.org/2014/04/what-the-scarcity-of-women-in-business-case-studies-really-looks-like”) 1.5.1.2 Discussion ideas What are the thoughts of the speed of the interaction? Did the animation help or hurt? What key idea did the authors want the reader to leave with from this visualization? What good make it better. 1.5.2 Our Problem We could develop two interactive methods to explore the problem. One for the tables and the other for the vector plots. 1.5.2.1 Interactive Tables We will generate several tables that a student can interact with in an html document. 1.5.2.1.1 First Table Our first attempt at interaction is to create a table where you can sort the columns. First we create the data frame. interdata&lt;-data.frame(MLB_long %&gt;% group_by(Player,Year,Result) %&gt;% summarise(Hits=n()) %&gt;% filter(Result==&quot;Hit&quot;) %&gt;% left_join(MLB_long %&gt;% group_by(Player,Year) %&gt;% summarise(At_Bats=n()),by=c(&quot;Player&quot;,&quot;Year&quot;)) %&gt;% select(Year, Player,Hits,At_Bats) %&gt;% arrange(Year)) %&gt;% rbind(.,group_by(.,Player) %&gt;% summarise(Hits=sum(Hits),At_Bats=sum(At_Bats)) %&gt;% mutate(Year=&quot;Combined&quot;) %&gt;% select(Year, Player, Hits, At_Bats) ) %&gt;% mutate(Average=round(Hits/At_Bats,4)) Next we generate the table using the DT package (Xie 2018b). DT::datatable(interdata) 1.5.2.1.2 Discussion Ideas Have the students experiment with the table. some examples are to use the sorting arrows to sort by player first then year or subset the data to only show Derek Jeter, note use the search window. What are the advantages and disadvantages of this interactive table? 1.5.2.1.3 Second Interactive Table For the second table we will use a pivot table using the rpivotTable package (Martoglio 2017). By default, we have fractions as a function of row totals. rpivotTable(MLB_long,rows=c(&quot;Year&quot;,&quot;Player&quot;),cols=&quot;Result&quot;,aggregatorName=&quot;Count as Fraction of Rows&quot;) 1.5.2.1.4 Discussion Ideas Have the students first remove year to get the batting averages for each player. Now ask the students to experiment with the pulldown menus to create other summaries; sort using the small areas next to the aggregation pulldown menu; switch year and player and determine the output. 1.5.2.2 Interactive Vector Plots Without developing our own custom visualization of the vector plot, which we could do in geogebra or using the R package shiny(Chang et al. 2018), we decided to find an existing application on the web. There is an interactive visualization application in the geogebra library, go to https://www.geogebra.org/m/YrPAV6KK. The example uses shooting baskets but gives the same idea as the reversal we saw in our problem. We selected this interaction because it helps explain why the reversal occurs and the conditions under which it occurs. 1.5.2.3 Discussion ideas Have the participants move Lucy1 around by experimenting with the length and angle to determine when the reversal occurs. What is the relationship between angle and the reversal? What is the relationship between length and reversal? Is it possible to have Tom as the better shooter regardless of Lucy’s actions as long as Lucy’s first and second day performances are less than Tom’s? What are the strengths of the interaction tool? What would have to be done to use it for our data? References "],
["final-product.html", "1.6 Final Product", " 1.6 Final Product The fourth phase of the analysis is the creation of a knowledge product. In this case the reader(s) will be varied. It is important to think about graphic design elements such as color, labels, font. The key point is to clearly define the message and then use the graphic elements to support this message. 1.6.1 Discussion Ideas What is the key message? What is the best method to communicate this message? Should we use a static graphic or an interactive display? 1.6.2 First Knowledge Product We think the key message is that Derek Jeter is the better batter than David Justice. We will use a static visual since interactive tools currently require access to the internet of special software. Before coming up with our final knowledge product we will discuss ideas using our vector plot from Figure 1.6, reproduced below. plotdata %&gt;% ggplot(aes(x=At_Bats,y=Hits)) + geom_point() + xlim(0,700)+ ylim(0,200)+ coord_fixed(ratio=1) + geom_segment(aes(x = 0, y = 0, xend = At_Bats, yend = Hits,color=Player),arrow=arrow(length=unit(.2,&#39;cm&#39;),type=&quot;closed&quot;)) + geom_text_repel(aes(x=At_Bats,y=Hits, label = Year)) + labs(title=&quot;Batting Averages&quot;) First let’s change the theme to reduce the background color. plotdata %&gt;% ggplot(aes(x=At_Bats,y=Hits)) + geom_point() + xlim(0,700)+ ylim(0,200)+ coord_fixed(ratio=1) + geom_segment(aes(x = 0, y = 0, xend = At_Bats, yend = Hits,color=Player),arrow=arrow(length=unit(.2,&#39;cm&#39;),type=&quot;closed&quot;)) + geom_text_repel(aes(x=At_Bats,y=Hits, label = Year)) + labs(title=&quot;Batting Averages&quot;)+ theme_minimal() Now let’s improve the labels. plotdata %&gt;% ggplot(aes(x=At_Bats,y=Hits)) + geom_point() + xlim(0,700)+ ylim(0,200)+ coord_fixed(ratio=1) + geom_segment(aes(x = 0, y = 0, xend = At_Bats, yend = Hits,color=Player),arrow=arrow(length=unit(.2,&#39;cm&#39;),type=&quot;closed&quot;)) + geom_text_repel(aes(x=At_Bats,y=Hits, label = Year)) + labs(title=&quot;Derek Jeter is Better than David Justice&quot;,x=&quot;Number of At Bats&quot;,y=&quot;Hits&quot;, subtitle=&quot;Batting averages for 1995 and 1996 as well as the combined averages&quot;)+ theme_minimal() Finally, let’s change the ticks on the axes and a different theme. plotdata %&gt;% ggplot(aes(x=At_Bats,y=Hits)) + geom_point() + ylim(0,200)+ coord_fixed(ratio=1) + geom_segment(aes(x = 0, y = 0, xend = At_Bats, yend = Hits,color=Player),arrow=arrow(length=unit(.2,&#39;cm&#39;),type=&quot;closed&quot;)) + geom_text_repel(aes(x=At_Bats,y=Hits, label = Year)) + labs(title=&quot;Derek Jeter is Better than David Justice&quot;,x=&quot;Number of At Bats&quot;,y=&quot;Hits&quot;, subtitle=&quot;Batting averages for 1995 and 1996 as well as the combined averages&quot;)+ theme_bw() + scale_x_continuous(breaks=seq(0,600,50)) 1.6.3 Discussion Ideas What do you like and dislike about the latest visual? Does it accomplish it goal of showing Derek Jeter is the better batter? What would you change? 1.6.4 Second Knowledge Product d=data.frame(x1=c(0,0), y2=c(0.3095,0.2704), y1=c(0,0), x2=c(.630,0.551), Player=c(&#39;Derek Jeter&#39;,&#39;David Justice&#39;), r=c(630,551)) ggplot(d) + geom_rect(aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=Player), color=&quot;black&quot;, alpha=0.5) + theme_classic() + scale_fill_brewer(palette=&quot;Paired&quot;) + facet_grid(. ~ Player) + scale_x_continuous(name=&quot;At Bats&quot;) + scale_y_continuous(name=&quot;Batting Average&quot;) + #geom_text(data=d, aes(x=x1+(x2-x1)/2, y=y1+(y2-y1)/2, label=r), size=4) + labs(title=&quot;Derek Jeter is a batter batter than David Justice&quot;, subtitle=&quot;Based on data from the years 1995 - 1996&quot;) + geom_text(aes(x=x1+(x2-x1)/2, y=y1-0.01, label=r)) + theme(axis.text.x = element_blank(),axis.ticks.x = element_blank(),legend.position = &quot;none&quot;) d=data.frame(x1=c(0,0), y2=c(0.3095,0.2704), y1=c(0,0), x2=c(.630,0.551), Player=c(&#39;Derek Jeter&#39;,&#39;David Justice&#39;), r=c(630,551)) ggplot(d) + geom_rect(aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=Player), color=&quot;black&quot;, alpha=0.5) + theme_classic() + scale_fill_brewer(palette=&quot;Paired&quot;) + facet_grid(. ~ Player) + scale_x_continuous(name=&quot;At Bats&quot;,breaks=c(0,.25,.5,.75),labels=c(&quot;0&quot;, &quot;250&quot;, &quot;500&quot;,&quot;750&quot;)) + scale_y_continuous(name=&quot;Batting Average&quot;) + labs(title=&quot;Derek Jeter is a batter batter than David Justice&quot;, subtitle=&quot;Based on data from the years 1995 - 1996&quot;) + theme(legend.position = &quot;none&quot;) 1.6.5 Web Resources The following are some resource https://www.r-bloggers.com/r-how-to-layout-and-design-an-infographic/ http://nandeshwar.info/data-visualization/how-to-create-infographics-in-r/ http://flowingdata.com/category/tutorials/ "],
["Conc1.html", "1.7 Conclusion", " 1.7 Conclusion In conclusion of this section go back over the objectives and ask participants to explain and discuss. Objectives After the training, participants will: be able to define and explain visual analytics be able to describe the three important components of visual analytics be able to discuss types of unique insights that VA can provide "],
["Chpt2.html", "Chapter 2 Exploratory Visual Analytics", " Chapter 2 Exploratory Visual Analytics “It isn’t accidental that when we begin to understand something we say ‘I see.’ Not ‘I hear’ or ‘I smell’” – Stephen Few “You see alot just by looking” – Yogi Berra “Visualization can surprise but doesn’t scale well. Modeling scales well but cannot surprise you.” – Hadley Wickham “We have the duty of formulating, of summarizing, and of communicating our conclusions, in intelligible form, in recognition of the right of other free minds to utilize them in making their own decisions” – Ronald Fisher In Chapter 1 we illustrated the visual analytics process in Figure 1.1. In that chapter we started in the analytic reasoning section and then proceeded to visualization and interaction. This is typical when you begin with a research question and perform a more traditional analysis. In this chapter, we will be conducting what is known as an explanatory analysis in a visual analytic framework. We will start with visualization and explore relationships between variables. Ultimately, we will be recreating a chart similar to Hans Rosling’s famous animated visualization from his TED talk. We will be using R to generate the display however, if you don’t want to use R go the gapminder website and under the header tools, you will have access to all the data as well as an interactive scatter plot. Another avenue you could explore is to use data directly from the World Bank. On their website you can create interactive visual displays. The beginning of this chapter has a heavy, but reasonable, amount of data acquisition and preparation. This is often minimized in a typical lesson but in practice this part of the analysis comprises a significant portion of the time and effort in the project. In the visualization section, we offer many different options for visualizing the data. Finally, in the interactive section, we suggest several methods to interact with the data. We are limited by what can be hosted in an html5 format. Many more options are available if you host on a server; however, as lesson the ideas are present in this section. Again, this chapter can be condensed into a smaller set of notes by dropping portions of each section. The data cleaning section is the main area where material can be removed; however, we caution that most learning experiences remove the data cleaning process leading students to an illusion that data is easy to find and work with. "],
["lesson-preparation-1.html", "2.1 Lesson Preparation", " 2.1 Lesson Preparation 2.1.1 Objectives: After the training, students will: be able to perform basic data manipulation and preparation be able to generate an animated visualization be able to evaluate and compare visual displays of data 2.1.2 Preparation Prior to class, complete the following Watch the video from Hans Rosling entitled “The best stats you’ve ever seen” . Go to the gapminder website and under the tab tools, experiment with the data. "],
["Intro2.html", "2.2 Lesson Introduction", " 2.2 Lesson Introduction Suggestions to start the lesson: In the video, Rosling mentions that his students believe that developed countries have long life expectancies and low birth rates while developing countries are the opposite. What are some other distinctions between these categories? There are many ways to answer this question and answers may include discussions about GPD, infant mortality rates, and age of population. One definition of developed country is To be considered a developed nation, a country generally has a per capita income around or above $12,000 This is an economic criteria. Other criteria can be found on Wikipedia. What do you think made Hans Rosling’s presentation so effective? What did you like about the visualizations? What are some of the design principles you learned from this video? We are replicating Figure 1.1 below to remind you of the process for visual analytics. We are assuming that you have read Chapter 1 of this book even if you did not use the material. After importing and cleaning the data, we will jump into the visualization portion of the cycle first in an attempt to find insight. We are looking for a way to determine developed and developing countries and watch their progression over time. Thus we will go to analytical reasoning and interaction after starting with the visualization. We must start with data collection first. "],
["DataCol2.html", "2.3 Data Collection", " 2.3 Data Collection The data for this lesson comes from the Gapminder website. Again, you could just start with the tools application on the website instead of using the R code here. This section will be long but remember that data wrangling can take up to 80% of the time in a project. We also provide the final data in a csv file on the github site for this book. The file name is Gapminder_final.csv. 2.3.1 Load Packages First we will load the packages we need for this chapter. The list below are the packages we will be using: library(readxl) library(gapminder) library(tidyverse) library(GGally) library(plotly) library(crosstalk) library(ggthemes) library(geofacet) library(DT) library(d3scatter) library(rbokeh) library(trelliscopejs) First data is downloaded in a Excel spreadsheet from the Gapminder website. We will use the package readxl to read it into R. In addition, R has a package called gapminder which is a subset of the data from the Gapminder website. We will use it to get country names to reduce the size of our data and exclude countries with a large amount of missing data. If you want to skip the data collection and cleaning steps, data has been downloaded and is in the data folder on the GitHub site for this book. 2.3.2 Population The first data set we will be working with is the population for each country. 2.3.2.1 Import Data First import the data. gapminder_population_temp &lt;- read_excel(&quot;data/indicator gapminder population.xlsx&quot;) Look at the first few rows of the data to get a feel for the data. Total population 1800.0 1810.0 1820.0 1830.0 1840.0 1850.0 1860.0 1870.0 1880.0 1890.0 1900.0 1910.0 1920.0 1930.0 1940.0 1950.0 1951.0 1952.0 1953.0 1954.0 1955.0 1956.0 1957.0 1958.0 1959.0 1960.0 1961.0 1962.0 1963.0 1964.0 1965.0 1966.0 1967.0 1968.0 1969.0 1970.0 1971.0 1972.0 1973.0 1974.0 1975.0 1976.0 1977.0 1978.0 1979.0 1980.0 1981.0 1982.0 1983.0 1984.0 1985.0 1986.0 1987.0 1988.0 1989.0 1990.0 1991.0 1992.0 1993.0 1994.0 1995.0 1996.0 1997.0 1998.0 1999.0 2000.0 2001.0 2002.0 2003.0 2004.0 2005.0 2006.0 2007.0 2008.0 2009.0 2010.0 2011.0 2012.0 2013.0 2014.0 2015.0 Abkhazia NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Afghanistan 3280000 3280000 3323519 3448982 3625022 3810047 3973968 4169690 4419695 4710171 5021241 5351413 5813814 6394908 7034081 7752118 7839426 7934798 8038312 8150037 8270024 8398309 8534913 8679848 8833127 8994793 9164945 9343772 9531555 9728645 9935358 10148841 10368600 10599790 10849510 11121097 11412821 11716896 12022514 12315553 12582954 12831361 13056499 13222547 13283279 13211412 12996923 12667001 12279095 11912510 11630498 11438949 11337932 11375768 11608351 12067570 12789374 13745630 14824371 15869967 16772522 17481800 18034130 18511480 19038420 19701940 20531160 21487079 22507368 23499850 24399948 25183615 25877544 26528741 27207291 27962207 28809167 29726803 30682500 31627506 32526562 Akrotiri and Dhekelia NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 10661 10737 10813 10889 10966 11043 11121 11200 11279 11358 11439 11519 11601 11683 11765 11848 11932 12016 12101 12187 12273 12359 12447 12535 12623 12712 12802 12892 12983 13075 13167 13260 13354 13448 13543 13639 13735 13832 13930 14028 14127 14227 14328 14429 14531 14633 14737 14841 14946 15051 15157 15264 15372 15481 15590 15700 15700 15700 15700 NA NA NA NA NA NA NA Albania 410445 423591 438671 457234 478227 506889 552800 610036 672544 741688 819950 901122 963956 1015991 1123210 1263171 1287499 1316086 1348097 1382881 1419969 1459089 1500152 1543224 1588478 1636054 1685901 1737645 1790533 1843596 1896125 1947786 1998695 2049147 2099657 2150602 2202040 2253842 2305999 2358467 2411229 2464338 2517869 2571845 2626290 2681245 2735329 2788315 2842620 2901590 2966799 3041003 3121336 3197064 3253659 3281453 3275438 3240613 3189623 3140634 3106727 3092034 3092471 3102898 3114851 3121965 3124093 3123112 3117045 3103758 3082172 3050741 3010849 2968026 2929886 2901883 2886010 2880667 2883281 2889676 2896679 Algeria 2503218 2595056 2713079 2880355 3082721 3299305 3536468 3811028 4143163 4525691 4946166 5404045 6063800 6876190 7797418 8872247 9039913 9216395 9405445 9609507 9829717 10065829 10316288 10578453 10848971 11124892 11404859 11690152 11985130 12295973 12626953 12980269 13354197 13744383 14144437 14550033 14960111 15377095 15804428 16247113 16709098 17190236 17690184 18212331 18760761 19337723 19943667 20575701 21228288 21893857 22565908 23241276 23917889 24591493 25257671 25912364 26554277 27180921 27785977 28362015 28904300 29411839 29887717 30336880 30766551 31183658 31590320 31990387 32394886 32817225 33267887 33749328 34261971 34811059 35401790 36036159 36717132 37439427 38186135 38934334 39666519 American Samoa 8170 8156 8142 8128 8114 7958 7564 7057 6582 6139 5949 7047 8173 10081 13135 18937 19295 19543 19683 19729 19706 19647 19597 19606 19729 20012 20478 21118 21883 22701 23518 24320 25116 25886 26615 27292 27916 28490 29014 29491 29932 30325 30690 31105 31670 32456 33488 34740 36165 37687 39247 40835 42448 44049 45591 47044 48379 49597 50725 51807 52874 53926 54942 55899 56768 57522 58176 58729 59117 59262 59117 58648 57904 57031 56226 55636 55316 55227 55302 55434 55538 From the gapminder package, obtain the subset of countries where the data is most complete. countries&lt;-levels(gapminder$country) Finally, wrangle the data into the final form needed. The term used is tidy data; each row is an observation and each column a variable. As currently imported, the data is in a wide format where the value of years, which is a single variable, are used as column titles. Switch to a long format. gapminder_population&lt;-gapminder_population_temp %&gt;% gather(year,pop,-&quot;Total population&quot;) %&gt;% rename(country=&quot;Total population&quot;) %&gt;% mutate(year=as.integer(year),pop=as.integer(pop)) %&gt;% filter(country %in% countries,year&gt;=1950) Table 2.1 is the first 6 rows of our data. Table 2.1: Population Data Arranged into Rows country year pop Afghanistan 1950 7752118 Albania 1950 1263171 Algeria 1950 8872247 Angola 1950 4354882 Argentina 1950 17150335 Australia 1950 8177344 2.3.2.2 Quality Check Before proceeding, check the condition of the data and clean up any obvious issues. First compare the number of countries in our data with those in the gapminder package. length(countries) ## [1] 142 length(unique(gapminder_population$country)) ## [1] 139 There are 142 countries in the gapminder data but only 139 in the data we collected that match those 142 countries. There might some names in gapminder that are different from the imported data. Check this idea first. countries[!(countries %in% unique(gapminder_population$country))] ## [1] &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; &quot;Yemen, Rep.&quot; We are missing the two Koreas and Yemen from our data. We need the full list of countries in the original data to change the names. gapminder_population&lt;-gapminder_population_temp %&gt;% gather(year,pop,-&quot;Total population&quot;) %&gt;% rename(country=&quot;Total population&quot;) %&gt;% mutate(year=as.integer(year),pop=as.integer(pop)) %&gt;% filter(year&gt;=1950) unique(gapminder_population$country)[grep(&quot;Korea&quot;,unique(gapminder_population$country))] ## [1] &quot;North Korea&quot; &quot;South Korea&quot; &quot;United Korea (former)&quot; unique(gapminder_population$country)[grep(&quot;Yemen&quot;,unique(gapminder_population$country))] ## [1] &quot;North Yemen (former)&quot; &quot;South Yemen (former)&quot; &quot;Yemen&quot; We now know the problem is that in the gapminder package we have the titles: “Korea, Dem. Rep.” “Korea, Rep.” “Yemen, Rep.” while in our data from the gapminder website we have the names: “North Korea” “South Korea” “Yemen” Correct the names in the our data. gapminder_population$country&lt;-gsub(&quot;North Korea&quot;,&quot;Korea, Dem. Rep.&quot;,gapminder_population$country) gapminder_population$country&lt;-gsub(&quot;South Korea&quot;,&quot;Korea, Rep.&quot;,gapminder_population$country) gapminder_population$country&lt;-gsub(&quot;Yemen&quot;,&quot;Yemen, Rep.&quot;,gapminder_population$country) Now check our data again. sum(!(countries %in% unique(gapminder_population$country))) ## [1] 0 Finally get rid of the countries not in the gapminder package. gapminder_population&lt;-gapminder_population %&gt;% filter(country %in% countries) Next look for missing observations. gapminder_population[is.na(gapminder_population$pop),] ## # A tibble: 2 x 3 ## country year pop ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Taiwan 2014 NA ## 2 Taiwan 2015 NA We are missing the last two years of data for Taiwan. We could drop Taiwan or get another estimate of the population from the web. Using the website http://www.worldometers.info/world-population/taiwan-population/ we see that for the last four years the population has grown by about 70000 each year. We will estimate the population for 2014 and 2015 by adding 70000 to the population in 2013 for each year.. gapminder_population[is.na(gapminder_population$pop),][1,3]=23151000 + 70000 gapminder_population[is.na(gapminder_population$pop),][1,3]=23151000 + 140000 2.3.2.3 Getting Data into Final Form Now add the continents names to the data frame. First get the country names and associated continent names from the gapminder package. dict&lt;-gapminder %&gt;% group_by(continent) %&gt;% distinct(country) %&gt;% mutate(country=as.character(country)) Add the continent names using a join function. gapminder_population &lt;- gapminder_population %&gt;% left_join(dict,by=&quot;country&quot;) %&gt;% select(country,continent,year,pop) Table 2.2 shows the form of the final data set. Table 2.2: Final Population Data country continent year pop Afghanistan Asia 1950 7752118 Albania Europe 1950 1263171 Algeria Africa 1950 8872247 Angola Africa 1950 4354882 Argentina Americas 1950 17150335 Australia Oceania 1950 8177344 To conclude, clean up the workspace by removing temporary objects. rm(gapminder_population_temp) 2.3.2.4 Discussion Ideas Even though the video we watched used population, what are the disadvantages of using raw population? A large country will have a larger population because of size, what is another population metric that places the data on an equal scale? 2.3.3 Population Density Adjusting population by the size of the country gives a better basis to compare countries. The gapminder website has a data set on population density, the population divided by the area of the country in squared kilometers, but it only goes to the year 2010. The gapminder website also has a data file with the area of each country. It also only goes up to 2010. To find population density, we will import the surface area file and divide population by the surface area. This file only goes up to 2010 so we will have to assume that the area does not change in the last five years. gapminder_area &lt;- read_excel(&quot;data/surface land.xlsx&quot;) gapminder_area &lt;- gapminder_area %&gt;% gather(year,area,-&quot;Surface area (sq. km)&quot;) %&gt;% rename(country=&quot;Surface area (sq. km)&quot;) %&gt;% mutate(year=as.integer(year)) %&gt;% filter(year &gt; 1960,year&lt;2011) Next check the country names. countries[!(countries %in% unique(gapminder_area$country))] ## [1] &quot;Central African Republic&quot; &quot;Czech Republic&quot; ## [3] &quot;Dominican Republic&quot; &quot;Reunion&quot; ## [5] &quot;Taiwan&quot; unique(gapminder_area$country)[grep(&quot;Czech&quot;,unique(gapminder_area$country))] ## [1] &quot;Czech Rep.&quot; unique(gapminder_area$country)[grep(&quot;Dominican&quot;,unique(gapminder_area$country))] ## [1] &quot;Dominican Rep.&quot; unique(gapminder_area$country)[grep(&quot;Central&quot;,unique(gapminder_area$country))] ## [1] &quot;Central African Rep.&quot; unique(gapminder_area$country)[grep(&quot;Reunion&quot;,unique(gapminder_area$country))] ## character(0) unique(gapminder_area$country)[grep(&quot;Taiwan&quot;,unique(gapminder_area$country))] ## character(0) We are missing Taiwan and Reunion from the data and for the other three mismatches we have Republic named as Rep. gapminder_area$country&lt;-gsub(&quot;Czech Rep.&quot;,&quot;Czech Republic&quot;,gapminder_area$country) gapminder_area$country&lt;-gsub(&quot;Central African Rep.&quot;,&quot;Central African Republic&quot;,gapminder_area$country) gapminder_area$country&lt;-gsub(&quot;Dominican Rep.&quot;,&quot;Dominican Republic&quot;,gapminder_area$country) We will have to add Taiwan and Reunion after we complete the data consolidation. gapminder_area&lt;-gapminder_area %&gt;% filter(country %in% countries) gapminder_area %&gt;% filter(is.na(area)) %&gt;% select(country) %&gt;% unique() ## # A tibble: 1 x 1 ## country ## &lt;chr&gt; ## 1 Belgium Belgium is missing data, so we will just fill just fill in the values with the value of 30530. gapminder_area %&gt;% filter(country==&quot;Belgium&quot;) %&gt;% filter(!is.na(area)) ## # A tibble: 11 x 3 ## country year area ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Belgium 2000 30530 ## 2 Belgium 2001 30530 ## 3 Belgium 2002 30530 ## 4 Belgium 2003 30530 ## 5 Belgium 2004 30530 ## 6 Belgium 2005 30530 ## 7 Belgium 2006 30530 ## 8 Belgium 2007 30530 ## 9 Belgium 2008 30530 ## 10 Belgium 2009 30530 ## 11 Belgium 2010 30530 gapminder_area &lt;- gapminder_area %&gt;% replace_na(list(area=30530)) Now merge the data with the population data and fill in the missing values. gapminder_popdensity &lt;- gapminder_population %&gt;% left_join(gapminder_area,by=c(&quot;country&quot;,&quot;year&quot;)) Fill in data for Taiwan and Reunion. Taiwan is missing its density. The area of Taiwan is 36,000 squared kilometers and the area of Reunion is 2510, . gapminder_popdensity[gapminder_popdensity$country==&quot;Reunion&quot;,&quot;area&quot;] = 2510 gapminder_popdensity[gapminder_popdensity$country==&quot;Taiwan&quot;,&quot;area&quot;] = 36000 Fill in the years prior to 1961 with the values from 1961. gapminder_popdensity&lt;- gapminder_popdensity %&gt;% group_by(country) %&gt;% fill(area,.direction=&quot;up&quot;) %&gt;% ungroup() %&gt;% arrange(year,country) Fill in the missing value in the tail with the most recent values. gapminder_popdensity&lt;- gapminder_popdensity %&gt;% group_by(country) %&gt;% fill(area,.direction=&quot;down&quot;) %&gt;% ungroup() %&gt;% arrange(year,country) Calculate density by dividing population by area. gapminder_popdensity &lt;- gapminder_popdensity %&gt;% mutate(density=pop/area) %&gt;% select(-area) Clean up the work space. rm(gapminder_area) rm(gapminder_population) 2.3.4 Life Expectancy Next bring in life expectancy data. Since the data source is the same as population we will have the same issue with the names of the Koreas and Yemen. We will be performing the same steps as section 2.3.2, so the narrative on many of the steps is skipped. gapminder_lifeexp_temp &lt;- read_excel(&quot;data/indicator life_expectancy_at_birth.xlsx&quot;) gapminder_lifeexp&lt;-gapminder_lifeexp_temp %&gt;% gather(year,life_exp,-&quot;Life expectancy&quot;) %&gt;% rename(country=&quot;Life expectancy&quot;) %&gt;% mutate(year=as.integer(year)) %&gt;% filter(year&gt;=1950) rm(gapminder_lifeexp_temp) gapminder_lifeexp$country&lt;-gsub(&quot;North Korea&quot;,&quot;Korea, Dem. Rep.&quot;,gapminder_lifeexp$country) gapminder_lifeexp$country&lt;-gsub(&quot;South Korea&quot;,&quot;Korea, Rep.&quot;,gapminder_lifeexp$country) gapminder_lifeexp$country&lt;-gsub(&quot;Yemen&quot;,&quot;Yemen, Rep.&quot;,gapminder_lifeexp$country) gapminder_lifeexp&lt;-gapminder_lifeexp %&gt;% filter(country %in% countries) sum(is.na(gapminder_lifeexp)) ## [1] 0 gapminder_lifeexp &lt;- gapminder_lifeexp %&gt;% left_join(dict,by=&quot;country&quot;) %&gt;% select(country,continent,year,life_exp) Again 2.3 shows the final data set. Table 2.3: Final Life Expectancy Data country continent year life_exp Afghanistan Asia 1950 26.85 Albania Europe 1950 54.48 Algeria Africa 1950 42.77 Angola Africa 1950 30.70 Argentina Americas 1950 61.61 Australia Oceania 1950 69.01 2.3.5 Fertility Rates Next fertility rates of a country may be an indication of its developmental state. Get the data. gapminder_fertility_temp &lt;- read_excel(&quot;data/indicator undata total_fertility.xlsx&quot;) gapminder_fertility&lt;-gapminder_fertility_temp %&gt;% gather(year,fertility,-&quot;Total fertility rate&quot;) %&gt;% rename(country=&quot;Total fertility rate&quot;) %&gt;% mutate(year=as.integer(year)) %&gt;% filter(year&gt;=1950) rm(gapminder_fertility_temp) gapminder_fertility$country&lt;-gsub(&quot;North Korea&quot;,&quot;Korea, Dem. Rep.&quot;,gapminder_fertility$country) gapminder_fertility$country&lt;-gsub(&quot;South Korea&quot;,&quot;Korea, Rep.&quot;,gapminder_fertility$country) gapminder_fertility$country&lt;-gsub(&quot;Yemen&quot;,&quot;Yemen, Rep.&quot;,gapminder_fertility$country) gapminder_fertility&lt;-gapminder_fertility %&gt;% filter(country %in% countries) length(unique(gapminder_fertility$country)) ## [1] 142 sum(is.na(gapminder_fertility$fertility)) ## [1] 2 gapminder_fertility[is.na(gapminder_fertility$fertility),] ## # A tibble: 2 x 3 ## country year fertility ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Taiwan 2014 NA ## 2 Taiwan 2015 NA Taiwan again has missing values. Estimate the fertility with average of the previous 5 years. temp&lt;-as.double(gapminder_fertility %&gt;% filter(country==&quot;Taiwan&quot;,year&lt;2014 &amp; year&gt;2008) %&gt;% summarise(ave=mean(fertility))) gapminder_fertility&lt;-gapminder_fertility %&gt;% replace_na(list(fertility=temp)) rm(temp) gapminder_fertility &lt;- gapminder_fertility %&gt;% left_join(dict,by=&quot;country&quot;) %&gt;% select(country,continent,year,fertility) Table 2.4 is the final form of the fertility data. Table 2.4: Final Fertility Data country continent year fertility Afghanistan Asia 1950 7.67 Albania Europe 1950 5.80 Algeria Africa 1950 7.65 Angola Africa 1950 6.93 Argentina Americas 1950 3.16 Australia Oceania 1950 3.07 2.3.6 Child Mortality Next collect data on child mortality. This data is the number of children out of 1000 born in the given year that will die before reaching the age of 5. gapminder_mortality5_temp &lt;- read_excel(&quot;data/indicator gapminder under5mortality.xlsx&quot;) gapminder_mortality5&lt;-gapminder_mortality5_temp %&gt;% gather(year,mortality,-&quot;Under five mortality&quot;) %&gt;% rename(country=&quot;Under five mortality&quot;) %&gt;% mutate(year=as.integer(year)) %&gt;% filter(year&gt;=1950) rm(gapminder_mortality5_temp) gapminder_mortality5$country&lt;-gsub(&quot;North Korea&quot;,&quot;Korea, Dem. Rep.&quot;,gapminder_mortality5$country) gapminder_mortality5$country&lt;-gsub(&quot;South Korea&quot;,&quot;Korea, Rep.&quot;,gapminder_mortality5$country) gapminder_mortality5$country&lt;-gsub(&quot;Yemen&quot;,&quot;Yemen, Rep.&quot;,gapminder_mortality5$country) gapminder_mortality5&lt;-gapminder_mortality5 %&gt;% filter(country %in% countries) sum(is.na(gapminder_mortality5$mortality)) ## [1] 156 gapminder_mortality5[is.na(gapminder_mortality5$mortality),] ## # A tibble: 156 x 3 ## country year mortality ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Hong Kong, China 1950 NA ## 2 Puerto Rico 1950 NA ## 3 Reunion 1950 NA ## 4 Taiwan 1950 NA ## 5 Hong Kong, China 1951 NA ## 6 Puerto Rico 1951 NA ## 7 Reunion 1951 NA ## 8 Taiwan 1951 NA ## 9 Hong Kong, China 1952 NA ## 10 Puerto Rico 1952 NA ## # ... with 146 more rows Now Hong Kong, Puerto Rico, and Reunion are missing data up through the year 1979 and Taiwan is missing all years. We could drop Taiwan from the data but at this point that is a waste of information from the other data sets. We could impute the values but we need to be careful when we interpret the resulting plots. The first three countries mentioned are going to pose problems for the years prior to 1980. For ease will will drop these countries from the analysis after we get our final data set. gapminder_mortality5 &lt;- gapminder_mortality5 %&gt;% left_join(dict,by=&quot;country&quot;) %&gt;% select(country,continent,year,mortality) The final data for child mortality is in Table 2.5 Table 2.5: Final Child Mortality Data country continent year mortality Afghanistan Asia 1950 439.56 Albania Europe 1950 165.17 Algeria Africa 1950 292.13 Angola Africa 1950 404.92 Argentina Americas 1950 93.53 Australia Oceania 1950 31.60 2.3.7 Aging Population Another idea is that developed countries have an older population by percentage. Collect data on the percent of population above 60 years in age. gapminder_above60_temp &lt;- read_excel(&quot;data/indicator_total above 60 percen.xlsx&quot;) gapminder_above60&lt;-gapminder_above60_temp %&gt;% gather(year,above_60,-&quot;Total above 60 (%)&quot;) %&gt;% rename(country=&quot;Total above 60 (%)&quot;) %&gt;% mutate(year=as.integer(year)) %&gt;% filter(year&gt;=1950) rm(gapminder_above60_temp) This data came from a different source, so be careful about the country names. countries[!countries %in% unique(gapminder_above60$country)] ## [1] &quot;Central African Republic&quot; &quot;Czech Republic&quot; ## [3] &quot;Dominican Republic&quot; &quot;Taiwan&quot; Taiwan is still a problem but now there are three other countries that are in the gapminder package and not in the new data set. This is a problem with naming similar to what we found when finding density. Correct those first three countries. unique(gapminder_above60$country)[grep(&quot;Czech&quot;,unique(gapminder_above60$country))] ## [1] &quot;Czech Rep.&quot; unique(gapminder_above60$country)[grep(&quot;Dominican&quot;,unique(gapminder_above60$country))] ## [1] &quot;Dominican Rep.&quot; unique(gapminder_above60$country)[grep(&quot;Central&quot;,unique(gapminder_above60$country))] ## [1] &quot;Central African Rep.&quot; gapminder_above60$country&lt;-gsub(&quot;Czech Rep.&quot;,&quot;Czech Republic&quot;,gapminder_above60$country) gapminder_above60$country&lt;-gsub(&quot;Central African Rep.&quot;,&quot;Central African Republic&quot;,gapminder_above60$country) gapminder_above60$country&lt;-gsub(&quot;Dominican Rep.&quot;,&quot;Dominican Republic&quot;,gapminder_above60$country) Remove Taiwan since we have no data on it. While at it, remove Hong Kong, Puerto Rico, and Reunion. We could drop all data prior to 1980 but this is 30 years of data. Instead, we chose to drop these four small countries. final_countries&lt;- countries[!countries %in% unique(gapminder_mortality5[is.na(gapminder_mortality5$mortality),]$country)] Remove the four countries. gapminder_above60&lt;-gapminder_above60 %&gt;% filter(country %in% final_countries) sum(is.na(gapminder_above60$above_60)) ## [1] 0 There are no missing values. The data is only for every 5 years and has projections past 2015. We will remove the projections and then interpolate for the remaining years. unique(gapminder_above60$year) ## [1] 1950 1955 1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010 2015 ## [15] 2020 2025 2030 2035 2040 2045 2050 First remove the projections. gapminder_above60 &lt;- gapminder_above60 %&gt;% filter(year &lt;= 2015) Before interpolating, plot two countries to get an idea of the data. p&lt;-gapminder_above60 %&gt;% filter(country==&quot;Albania&quot; | country==&quot;United States&quot;) %&gt;% ggplot(aes(x=year,y=above_60,color=country))+ geom_point() p We will be using a spline to interpolate, but first plot a linear interpolation to compare with the spline. temp1&lt;-data.frame(with(gapminder_above60 %&gt;% filter(country==&quot;Albania&quot;),approx(year,above_60,xout=seq(1950,2015)))) %&gt;% mutate(country=&quot;Albania&quot;) temp2&lt;-data.frame(with(gapminder_above60 %&gt;% filter(country==&quot;United States&quot;),approx(year,above_60,xout=seq(1950,2015)))) %&gt;% mutate(country=&quot;United States&quot;) p+ geom_line(data=temp1,mapping=aes(x=x,y=y)) + geom_line(data=temp2,mapping=aes(x=x,y=y)) + labs(x=&quot;Year&quot;,y=&quot;Percentage of Population above 60 years&quot;,title=&quot;Estimating Percent of Population Above 60&quot;,subtitle=&quot;Linear Interpolation&quot;) Now use a spline. This process fits a cubic spline between each point and will be smoother than the linear interpolation. temp1&lt;-data.frame(with(gapminder_above60 %&gt;% filter(country==&quot;Albania&quot;),spline(year,above_60,xout=seq(1950,2015)))) %&gt;% mutate(country=&quot;Albania&quot;) temp2&lt;-data.frame(with(gapminder_above60 %&gt;% filter(country==&quot;United States&quot;),spline(year,above_60,xout=seq(1950,2015)))) %&gt;% mutate(country=&quot;United States&quot;) p+ geom_line(data=temp1,mapping=aes(x=x,y=y)) + geom_line(data=temp2,mapping=aes(x=x,y=y)) + labs(x=&quot;Year&quot;,y=&quot;Percentage of Population above 60 years&quot;,title=&quot;Estimating Percent of Population Above 60&quot;,subtitle=&quot;Spline Interpolation&quot;) Clean the work space. rm(temp1) rm(temp2) To avoid the temporary variables, use the code below in one step. p+ data.frame(with(gapminder_above60 %&gt;% filter(country==&quot;Albania&quot;),spline(year,above_60,xout=seq(1950,2015)))) %&gt;% mutate(country=&quot;Albania&quot;) %&gt;% geom_line(mapping=aes(x=x,y=y)) + data.frame(with(gapminder_above60 %&gt;% filter(country==&quot;United States&quot;),spline(year,above_60,xout=seq(1950,2015)))) %&gt;% mutate(country=&quot;United States&quot;) %&gt;% geom_line(mapping=aes(x=x,y=y)) + labs(x=&quot;Year&quot;,y=&quot;Percentage of Population above 60 years&quot;, title=&quot;Estimating Percent of Population Above 60&quot;,subtitle=&quot;Spline Interpolation&quot;) Interpolate for every country and create our final data frame. Use the map function from the purrr package. gapminder_above60&lt;-gapminder_above60 %&gt;% split(.$country) %&gt;% map(~data.frame(spline(.$year,.$above_60,xout=seq(1950,2015)))) %&gt;% map2_df(final_countries,~update_list(.x,country=.y)) %&gt;% mutate(year=as.integer(x),above_60=y) %&gt;% select(country,year,above_60) %&gt;% as.tibble() Finish by adding the continents. gapminder_above60 &lt;- gapminder_above60 %&gt;% left_join(dict,by=&quot;country&quot;) %&gt;% select(country,continent,year,above_60) %&gt;% arrange(year,country) The first few rows of the final data frame: gapminder_above60 %&gt;% head() %&gt;% knitr::kable() country continent year above_60 Afghanistan Asia 1950 4.503620 Albania Europe 1950 10.131796 Algeria Africa 1950 6.808316 Angola Africa 1950 4.944525 Argentina Americas 1950 7.043321 Australia Oceania 1950 12.458937 Finally, clean up the figure by removing the background shading. p+ geom_line(data=gapminder_above60%&gt;%filter(country==&quot;Albania&quot;),mapping=aes(x=year,y=above_60)) + geom_line(data=gapminder_above60%&gt;%filter(country==&quot;United States&quot;),mapping=aes(x=year,y=above_60)) + labs(x=&quot;Year&quot;,y=&quot;Percentage of Population above 60 years&quot;,title=&quot;Estimating Percent of Population Above 60&quot;,subtitle=&quot;Spline Interpolation&quot;) +theme_classic() 2.3.8 Final Data First do a quick quality check. table(gapminder_popdensity$continent)/66 ## ## Africa Americas Asia Europe Oceania ## 52 25 33 30 2 table(gapminder_lifeexp$continent)/67 ## ## Africa Americas Asia Europe Oceania ## 52 25 33 30 2 table(gapminder_fertility$continent)/66 ## ## Africa Americas Asia Europe Oceania ## 52 25 33 30 2 table(gapminder_mortality5$continent)/66 ## ## Africa Americas Asia Europe Oceania ## 52 25 33 30 2 table(gapminder_above60$continent)/66 ## ## Africa Americas Asia Europe Oceania ## 51 24 31 30 2 table(dict$continent) ## ## Africa Americas Asia Europe Oceania ## 52 25 33 30 2 We have two problems left, the percentage of the population above 60 has less countries than the other data sets and life expectancy has one more year, 2016, than the other data sets. Resolve these problems by dropping the 4 countries and the year 2016. gapminder_popdensity %&gt;% summarise(Total=n_distinct(country)) ## # A tibble: 1 x 1 ## Total ## &lt;int&gt; ## 1 142 gapminder_above60 %&gt;% summarise(Total=n_distinct(country)) ## # A tibble: 1 x 1 ## Total ## &lt;int&gt; ## 1 138 unique(gapminder_popdensity$country)[!unique(gapminder_popdensity$country) %in% final_countries] ## [1] &quot;Hong Kong, China&quot; &quot;Puerto Rico&quot; &quot;Reunion&quot; ## [4] &quot;Taiwan&quot; Drop the countries by using the join function. gapminder_va &lt;-gapminder_above60 %&gt;% left_join(gapminder_lifeexp,by=c(&quot;country&quot;,&quot;year&quot;,&quot;continent&quot;)) %&gt;% left_join(gapminder_fertility,by=c(&quot;country&quot;,&quot;year&quot;,&quot;continent&quot;)) %&gt;% left_join(gapminder_popdensity,by=c(&quot;country&quot;,&quot;year&quot;,&quot;continent&quot;)) %&gt;% left_join(gapminder_mortality5,by=c(&quot;country&quot;,&quot;year&quot;,&quot;continent&quot;)) %&gt;% arrange(year,country) %&gt;% mutate(country=factor(country),pop=round(pop/100000,2), above_60=round(above_60,2), mortality=round(mortality,2),density=round(density,2)) Look at the data. gapminder_va %&gt;% head() %&gt;% knitr::kable() country continent year above_60 life_exp fertility pop density mortality Afghanistan Asia 1950 4.50 26.85 7.67 77.52 11.89 439.56 Albania Europe 1950 10.13 54.48 5.80 12.63 43.94 165.17 Algeria Africa 1950 6.81 42.77 7.65 88.72 3.73 292.13 Angola Africa 1950 4.94 30.70 6.93 43.55 3.49 404.92 Argentina Americas 1950 7.04 61.61 3.16 171.50 6.17 93.53 Australia Oceania 1950 12.46 69.01 3.07 81.77 1.06 31.60 Finally, write the data to a csv file. This file could be used in the future without have to use the long data processing steps. write.csv(gapminder_va,file=&quot;data/Gapminder_final.csv&quot;) 2.3.9 Economic Data One last note is that we have not included an economic metric such as gross domestic product, GDP, per capita. The reason is that the data is sparse. On the gapminder website the data starts in 1960 and ends in 2011; in addition many countries are missing data. On the World Bank site, the data is more complete but still has many missing values and goes from 1960 to 2016. The data in the gapminder package in R has data for each country but only in five year increments and ends in 2007. if we want to include GDP data we would have to make the choice of limiting the number of years, interpolating, and/or removing more countries. It would be too much to extrapolate from 2007 to 2015, so a possible solution is to merge the World Bank data with the data from the gapminder package. However, they are inflation adjusted on different scales so a correction would have to be made. The World Bank data was in current US dollars while the gapminder package used something earlier, maybe 2000? Here is some code to investigate how to incorporate GDP, but we will not complete it for our work. It is only a start to give an idea of the work involved. gapminder_gdp_temp &lt;- read_excel(&quot;data/World Bank GDP.xlsx&quot;,skip=4) gapminder_gdp&lt;-gapminder_gdp_temp %&gt;% gather(year,gdpPercap,-(&quot;Country Name&quot;:&quot;Indicator Code&quot;)) %&gt;% select(-(`Country Code`:`Indicator Code`)) %&gt;% rename(country=&quot;Country Name&quot;) %&gt;% mutate(year=as.integer(year)) There is a problem with the names of the countries again. gdp_countries&lt;-unique(gapminder_gdp$country) countries[!countries %in% gdp_countries] ## [1] &quot;Egypt&quot; &quot;Gambia&quot; &quot;Hong Kong, China&quot; ## [4] &quot;Iran&quot; &quot;Korea, Dem. Rep.&quot; &quot;Reunion&quot; ## [7] &quot;Syria&quot; &quot;Taiwan&quot; &quot;Venezuela&quot; Determine which countries are in the new data set and then correct the names. ind&lt;-vector() for (country in final_countries[!final_countries %in% gdp_countries]){ ind&lt;-c(ind,grep(substr(country,1,6),gdp_countries)) } gdp_countries[ind] ## [1] &quot;Egypt, Arab Rep.&quot; &quot;Gambia, The&quot; ## [3] &quot;Iran, Islamic Rep.&quot; &quot;Korea, Rep.&quot; ## [5] &quot;Korea, Dem. People’s Rep.&quot; &quot;Syrian Arab Republic&quot; ## [7] &quot;Venezuela, RB&quot; Fix the names. We know Korea, Rep. is correct, let’s remove it. ind&lt;-ind[-4] #Two vectors of names to run a loop over old_names&lt;-gdp_countries[ind] new_names&lt;-final_countries[!final_countries %in% gdp_countries] for (i in seq_along(new_names)){ gapminder_gdp$country&lt;-gsub(old_names[i],new_names[i],gapminder_gdp$country) } gapminder_gdp&lt;-gapminder_gdp %&gt;% filter(country %in% final_countries,year&gt;=1992,year&lt;2017) %&gt;% arrange(country,year) Add continents to the data set. gapminder_gdp &lt;- gapminder_gdp %&gt;% left_join(dict,by=&quot;country&quot;) %&gt;% select(country,continent,year,gdpPercap) %&gt;% arrange(year,country) Now get the data from the gapminder package and compare the years to determine how to adjust. head(gapminder_gdp %&gt;% filter(year == 2007) %&gt;% select(gdpPercap)/ gapminder %&gt;% filter(year == 2007,country %in% final_countries) %&gt;% select(gdpPercap) - (gapminder_gdp %&gt;% filter(year == 2002) %&gt;% select(gdpPercap)/ gapminder %&gt;% filter(year == 2002,country %in% final_countries) %&gt;% select(gdpPercap))) ## gdpPercap ## 1 0.1210036 ## 2 0.2911512 ## 3 0.2968434 ## 4 0.3436639 ## 5 0.2697395 ## 6 0.5360116 The adjustment is not consistent so we may be lacking some understanding of how inflation adjustment is being done. Stop the process here but if you wanted to continue, you would adjust the data from the World Bank to be consistent with the gapminder data and then use interpolation similar to what was done with percentage of population above 60. Clean our work space. rm(gapminder_gdp_temp) rm(gapminder_gdp) rm(new_names) rm(old_names) 2.3.10 Summary of Data Collection To recap the data collection process, we will explain each variable. There are 138 countries on 5 continents for the years 1950 through and including 2015. Table 2.6 gives the first six rows of the data. Table 2.6: The First Rows of our Final Data Set country continent year above_60 life_exp fertility pop density mortality Afghanistan Asia 1950 4.50 26.85 7.67 77.52 11.89 439.56 Albania Europe 1950 10.13 54.48 5.80 12.63 43.94 165.17 Algeria Africa 1950 6.81 42.77 7.65 88.72 3.73 292.13 Angola Africa 1950 4.94 30.70 6.93 43.55 3.49 404.92 Argentina Americas 1950 7.04 61.61 3.16 171.50 6.17 93.53 Australia Oceania 1950 12.46 69.01 3.07 81.77 1.06 31.60 Table 2.7 summarizes the variables in our final data set. Table 2.7: Description of Variables Variable Description above_60 The percentage of the total population in a country that is above 60 years of age life_exp The average number of years a newborn child would live if current mortality patterns were to stay the same fertility The number of children that would be born to each woman with prevailing age-specific fertility rates pop The population of the country (100,000 people) density The population of the country divided by the area of the country mortality The probability that a child born in a specific year will die before reaching the age of five if subject to current age-specific mortality rates. Expressed as a rate per 1,000 live births In the next section, we will explore the process of visualizing the data. "],
["Viz2.html", "2.4 Visualization of Data", " 2.4 Visualization of Data In the next phase of our exploratory data analysis we will be visualizing the gapminder data. In Section 1.4, we discussed some of the ideas of visualization. As a first step, we will be generating a visualization to help us understand the content of the data. This step is important in exploration as it often leads to insights into the next step of the analysis. 2.4.1 First Visualizations In the first visualization we attempt to understand the variables and their relationship to each other. There are nine total variables. The first three variables, country, continent, and year are different from the last six and will have to be dealt with in a different manner. Country and continent are label variables. Year is a discrete count variable. The other six variables are continuous and can be plotted against each other on a scatterplot. 2.4.1.1 Discussion Ideas: What are the first steps we should take to understand and visualize the data? Are we doing reconnaissance or exploration, and what does that mean for the type of visualizations we want to create? We are doing exploration. We need to focus on detection of interesting features. Our choice of plots and colors should be driven by this goal. 2.4.1.2 Summarize Data Complete a descriptive summary of the data. str(gapminder_va) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 9108 obs. of 9 variables: ## $ country : Factor w/ 138 levels &quot;Afghanistan&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 4 1 1 2 5 4 3 3 4 ... ## $ year : int 1950 1950 1950 1950 1950 1950 1950 1950 1950 1950 ... ## $ above_60 : num 4.5 10.13 6.81 4.94 7.04 ... ## $ life_exp : num 26.9 54.5 42.8 30.7 61.6 ... ## $ fertility: num 7.67 5.8 7.65 6.93 3.16 3.07 1.87 7.03 6.23 2.3 ... ## $ pop : num 77.5 12.6 88.7 43.5 171.5 ... ## $ density : num 11.89 43.94 3.73 3.49 6.17 ... ## $ mortality: num 439.6 165.2 292.1 404.9 93.5 ... summary(gapminder_va) ## country continent year above_60 ## Afghanistan: 66 Africa :3366 Min. :1950 Min. : 1.620 ## Albania : 66 Americas:1584 1st Qu.:1966 1st Qu.: 4.860 ## Algeria : 66 Asia :2046 Median :1982 Median : 6.030 ## Angola : 66 Europe :1980 Mean :1982 Mean : 8.594 ## Argentina : 66 Oceania : 132 3rd Qu.:1999 3rd Qu.:11.092 ## Australia : 66 Max. :2015 Max. :33.040 ## (Other) :8712 ## life_exp fertility pop density ## Min. :13.20 Min. :1.130 Min. : 0.58 Min. : 0.50 ## 1st Qu.:51.70 1st Qu.:2.450 1st Qu.: 29.27 1st Qu.: 14.61 ## Median :62.90 Median :4.750 Median : 76.25 Median : 44.44 ## Mean :61.42 Mean :4.515 Mean : 321.45 Mean : 117.35 ## 3rd Qu.:72.20 3rd Qu.:6.450 3rd Qu.: 210.96 3rd Qu.: 108.53 ## Max. :83.30 Max. :9.220 Max. :13760.49 Max. :7892.59 ## ## mortality ## Min. : 2.0 ## 1st Qu.: 25.6 ## Median : 82.3 ## Mean :112.6 ## 3rd Qu.:180.0 ## Max. :500.4 ## There are some potential issues with this data in terms of visualizing. First population and density have a large range; this may require a logarithm scale. The variables country and continent are categorical and must be treated differently than the continuous variables. Finally, year is a count variable and we will have to deal with the 65 year span of data. 2.4.1.3 Matrix of Scatterplots To get an overview of the data, plot the pairwise scatterplots for each of the continuous variables. We still need to address the issue with year, but for now we will just pick the year 1950. ggpairs(gapminder_va %&gt;% filter(year==1950), columns = 4:9) Figure 2.1: Pairwise Scatterplots of Variables 2.4.1.4 Discussion Ideas Using Figure 2.1, discuss the following questions: What are advantages and disadvantages of the matrix scatterplot? What issues are revealed in the plot? What are some of the important conclusions that can be draws at this point in the exploration? What are some of the next steps that should be taken? These plots do a nice job of comparing variables however it is restricted to pairwise comparisons. The correlation coefficient summaries the linear relationship between variables but the relationship between life expectancy and fertility may be non-linear. The density plots along the diagonal illustrate the data are at least bimodal, this may be an important insight if we want to separate countries into a binary distinction such as developed and developing. The variables population and density are dominated by a few very large values. It may be advantageous to link or brush data points to track them across plots. The background lines in the upper triangle of the matrix is confusing and makes reading difficult. 2.4.1.5 Adjustments Put population and density on a logarithm scale. gapminder_va_log &lt;- gapminder_va %&gt;% mutate(log_pop=log10(pop),log_density=log10(density)) %&gt;% select(-pop,-density) Table 2.8: Data with Population and Density on a Log Scale country continent year above_60 life_exp fertility mortality log_pop log_density Afghanistan Asia 1950 4.50 26.85 7.67 439.56 1.89 1.08 Albania Europe 1950 10.13 54.48 5.80 165.17 1.10 1.64 Algeria Africa 1950 6.81 42.77 7.65 292.13 1.95 0.57 Angola Africa 1950 4.94 30.70 6.93 404.92 1.64 0.54 Argentina Americas 1950 7.04 61.61 3.16 93.53 2.23 0.79 Australia Oceania 1950 12.46 69.01 3.07 31.60 1.91 0.03 Next plot this new data set and add continent as a color. Figure 2.2: Pairwise Scatterplots of Variables (Population and Density on Log Scale) 2.4.1.6 Discussion Ideas What is wrong with the mortality density plot? Which variables seem to have the potential to separate the countries into developed and developing ? Why is the point cloud for the Americas so wide? Since Oceania only has two data points it is causing problems with the visual representation. We could continue with the exploration as is or combine Oceania with Europe since by most measures, except density, it is similar to Europe. The Americas combine North, Central, and South America together and the countries in this region have a wide range of values for the different variables. We may want to break them apart but that is only if we keep the analysis at the continent level and not the country level. 2.4.1.7 Recoding Oceania Combine Oceania into Europe; also check that our work is correct. table(gapminder_va_log$continent) ## ## Africa Americas Asia Europe Oceania ## 3366 1584 2046 1980 132 gapminder_va_log$continent &lt;- recode_factor(gapminder_va_log$continent,&#39;Oceania&#39;=&#39;Europe&#39;) table(gapminder_va_log$continent) ## ## Europe Africa Americas Asia ## 2112 3366 1584 2046 Now recreate the plot and add a legend and transparency to aid in visualization. Figure 2.3: Pairwise Scatterplots of Variables with Oceania Removed 2.4.1.8 Discussion Ideas Do you see the bimodal feature we saw in previous plots? What are your thoughts on life expectancy versus mortality? What is the next step? It is now easier to see that a large part of the bimodal features were due to differences in continents. However, in the Americas we still see this bimodal nature. It is expected that there is a strong linear relationship between life expectancy and mortality since they are related and do not offer different view of the population. The next step is to bring in the variable year. We should focus on two variables. The pairs that show promise are above 60 and fertility or mortality and fertility. We can bring population or density into the plots as well. 2.4.2 Next Visualizations - Adding Years and Focussing To understand the progression of countries over time, look at different years. In the previous section we looked only at the year 1950. In this section we will address different years. 2.4.2.1 Adding the Year 2015 First look at the scatterplot matrix for the final year, 2015. Figure 2.4: Pairwise Scatterplots of Variables for 2015 2.4.2.2 Discussion Ideas What changes do we see in the data compared to 1950? What features still standout in the plots? The countries of Asia, in terms of fertility and life expectancy, are now getting closer to Europe but with a large spread. The Americas still have the bimodal nature and Africa still lags the rest of the world. However, in general it appears countries are moving towards long life expectancy and low fertility. The variable above 60 is much different for Europe compared to the rest of the world. It is hard to compare the 2015 plot with the 1950 since they are separate plots. We need to have a side by side comparison. 2.4.2.3 Side by Side Comparison of 1950 and 2015 To compare two years, focus first on just two variables, fertility and life expectancy. If we want to minimize ink, we can use a theme based on Tufte’s work. ggplot(gapminder_va_log %&gt;% filter(year==2015 | year==1950),aes(x=life_exp,y=fertility,color=continent)) + geom_point(alpha=.7)+ facet_wrap(~year) + theme_tufte() + labs(x=&quot;Life Expectancy&quot;,y=&quot;Fertility&quot;,color=&quot;Continent&quot;) Figure 2.5: Side by Side Scatterplots for 1950 and 2015 2.4.2.4 Discussion Ideas How should the variable population be incorporated? What insights are gained from this visualization? Did changing the background theme help in the analysis? It is clear from these plots that in general fertility rates are going down and life expectancy is increasing. There is less spread in the data as most countries are in the lower right corner of the 2015 plot. Africa still lags the rest of the world but it is also showing gains. The background changed helped to visualize the data by removing the dark background. Making the data points a little translucent helped but maybe adding some jitter would also help. Population has not been incorporated but could be added by using size for population or density. 2.4.2.5 Population We can account for population by assigning it to another aesthetic such as size. This will work better if we go back to original data. Using population density will also be better than population because this variable accounts for the size of the country. First, combine Oceania with Europe. gapminder_va$continent &lt;- recode_factor(gapminder_va$continent,&#39;Oceania&#39;=&#39;Europe&#39;) One concern is that population and density will have an extreme skewness with a few large values. Let’s summarize population and density. gapminder_va %&gt;% filter(year==2015 | year==1950) %&gt;% select(density,pop) %&gt;% summary() ## density pop ## Min. : 0.50 Min. : 0.60 ## 1st Qu.: 13.18 1st Qu.: 26.12 ## Median : 48.84 Median : 72.00 ## Mean : 127.93 Mean : 336.42 ## 3rd Qu.: 104.90 3rd Qu.: 216.04 ## Max. :7892.59 Max. :13760.49 This confirms our suspicion. A country with a large population may not be one with a large density. ggplot(gapminder_va %&gt;% filter(year==2015 | year==1950),aes(x=density,y=pop,color=continent)) + geom_jitter() + facet_wrap(~year) + theme_tufte() Figure 2.6: Side by Side Scatterplots of Density and Population for 1950 and 2015 From Figure 2.6 it is clear that density and population are dominated by a few large values. We will need to be careful when we use either as an aesthetic on our plots. We will break the size into a logarithmic type of scale. We are also going to change the color scheme to one from the Brewer color scheme. ggplot(gapminder_va %&gt;% filter(year==2015 | year==1950),aes(x=life_exp,y=fertility,color=continent,size=density)) + geom_point(alpha=.7) + scale_size(name=&quot;Density&quot;, breaks=c(10,100,1000,4000,6000),range=c(1,7)) + scale_colour_brewer(palette=&quot;Dark2&quot;,name=&quot;Continent&quot;) + facet_wrap(~year) + theme_tufte() + labs(x=&quot;Life Expectancy&quot;,y=&quot;Fertility&quot;) Figure 2.7: Side by Side Scatterplots for 1950 and 2015 with Population Density We are getting tired of adding the theme element so we will just set it as the default. theme_set(theme_tufte()) 2.4.2.6 Discussion Questions Using Figure 2.7 answer the following questions: What is happening to population density over the 65 years? Which continent has had the greatest increase in life expectancy? Which country has had the greatest increase in life expectancy? It is clear that as population increases from 1950 to 2015, the population density increases. The high density in 2015 of a few countries makes most of the data points seem to be the same size. 2.4.2.7 Adding a Third Year, 1982 We still have not adequately incorporated year into the visual exploration. As a next step, add a year that is between the end points, in this case 1982. ggplot(gapminder_va %&gt;% filter(year==1982 |year==2015 | year==1950),aes(x=life_exp,y=fertility,color=continent,size=density)) + geom_point(alpha=.7) + scale_size(name=&quot;Density&quot;, breaks=c(10,100,1000,4000,6000),range=c(1,7)) + scale_colour_brewer(palette=&quot;Dark2&quot;,name=&quot;Continent&quot;) + facet_wrap(~year) + labs(x=&quot;Life Expectancy&quot;,y=&quot;Fertility&quot;) Figure 2.8: Side by Side Scatterplots for 1950, 1982, and 2015 with Population Density 2.4.2.8 Discussion Questions Using Figure 2.8 as a reference: What features do you see in the data? How should we incorporate year in a better format? The shift from 1950 to 1982 was mostly an increase in life expectancy with a moderate decrease in fertility. although for some of the African countries, the fertility increased. The period from 1982 to 2015 saw a big decrease in fertility with a minor increase in life expectancy. We could incorporate year by have a slider function to select year or another option is to animate the plot by year. 2.4.2.9 Using the Percentage of the Population Above 60 In our initial exploration, we also considered the variable population percentage above the age of 60. ggplot(gapminder_va %&gt;% filter(year==1982 |year==2015 | year==1950),aes(y=above_60,x=fertility,color=continent,size=density)) + geom_point(alpha=.7) + scale_size(name=&quot;Density&quot;, breaks=c(10,100,1000,4000,6000),range=c(1,7)) + scale_colour_brewer(palette=&quot;Dark2&quot;,name=&quot;Continent&quot;) + facet_wrap(~year) + labs(x=&quot;Fertility&quot;,y=&quot;Percentage of Population above 60&quot;) Figure 2.9: Side by Side Scatterplots of Fertility and Percentage of Population above 60 for 1950, 1982, and 2015 with Population Density 2.4.2.10 Discusssion Ideas From Figure 2.9 answer the following questions: What new insights are gained from this plot? What are some of the trends? Again, we see that in 1950 Europe is different from the other continents with a low fertility rate and a higher percentage of the population over 60. We do not see a gain in the percentage of elderly population in Africa over the entire time span. This could be due to the high fertility rate. In some sense, the percentage of the population over 60 incorporates both life expectancy and fertility. The fertility is going down in Africa but the percentage of the population over 60 is not changing. The continent with the biggest change over the 65 years is Asia. Again, it is hard to identify individual countries but we see an increase in population density. The change in density in Europe is not significant. We need the capability to brush and identify. In 2015 one Asian country has high percentage of population above 60. 2.4.2.11 Box and Dot Plots An alternative way to view the data is with a dotplot and boxplot. Start with a dotplot where the country is on the vertical axis and life expectancy is on the horizontal axis. Figure 2.10 gives the result. The vertical axis is too crowded and not readable even though we reduced the default font size. Notice also how the countries within continents are arranged in decreasing life expectancy. ggplot(gapminder_va %&gt;% filter(year==1982 |year==2015 | year==1950),aes(x=life_exp,y=reorder(country,life_exp),color=factor(year),size=density)) + geom_point() + scale_size(name=&quot;Density&quot;, breaks=c(10,100,1000,4000,6000),range=c(1,4)) + scale_color_discrete(name=&quot;Year&quot;) + facet_grid(continent~.,scale= &#39;free_y&#39;, space = &#39;free_y&#39;) + labs(x=&quot;Life Expectancy&quot;,y=&quot;&quot;) + theme_tufte(base_size = 8) Figure 2.10: Plots of Life Expectancy for Each Country grouped by Continent for 1950, 1982, and 2015 with Population Density We could break each plot down into an individual continent. For example Figure 2.11 shows the plot for Europe. This plot is much easier to read but we lose the ability to compare the other three continents. ggplot(gapminder_va %&gt;% filter(year==1982 |year==2015 | year==1950,continent==&quot;Europe&quot;),aes(x=life_exp,y=reorder(country,life_exp),color=factor(year),size=density)) + geom_point() + scale_size(name=&quot;Density&quot;, breaks=c(10,100,1000,4000,6000),range=c(1,4)) + scale_color_discrete(name=&quot;Year&quot;) + labs(x=&quot;Life Expectancy&quot;,y=&quot;&quot;) Figure 2.11: Plot of Life Expectancy for Europe for the Years 1950, 1982, and 2015 A third attempt is to make a two dimensional matrix plot of the four individual plots. Figure 2.12 shows the results. Africa is still a bit crowded but individual country names are visible. ggplot(gapminder_va %&gt;% filter(year==1982 |year==2015 | year==1950), aes(x=life_exp,y=reorder(country,life_exp),color=factor(year),size=density)) + geom_point() + scale_size(name=&quot;Density&quot;, breaks=c(10,100,1000,4000,6000),range=c(1,4)) + scale_color_discrete(name=&quot;Year&quot;) + facet_wrap(~continent,scales= &#39;free_y&#39;) + labs(x=&quot;Life Expectancy&quot;,y=&quot;&quot;) Figure 2.12: Matrix Plot of Life Expectancy for the Years 1950, 1982, and 2015 Another approach to compare continents is to generate boxplots for each continent for a particular year. Figure 2.13 is an example. This makes it easy to compare the continents withing a year but more difficult to compare across years. ggplot(gapminder_va %&gt;% filter(year==1950 | year == 2015), aes(y=life_exp,x=reorder(continent,life_exp))) + geom_boxplot() + coord_flip() + facet_grid(~year) + labs(y=&quot;Life Expectancy&quot;,x=&quot;Continent&quot;) Figure 2.13: Boxplots of Life Expectancy for Each Continent for the Years 1950 and 2015 2.4.2.12 Discussion Ideas What are the advantages of the boxplots versus the dotplots? What trends do we see in the boxplots? How can we deal with the variable year in a better manner? What are some ways to address the issue of the crowded vertical axes in the dotplots? In the boxplot we get an idea of the variability of life expectancy within each continent. It is easy to compare continents as well. However, we don’t have the ability to analyze on the country level. In general, we see that life expectancy has improved for all continents. Europe still has the best results but the difference between continents in 2015 is much smaller than it was in 1950. The variability is also decreasing as the width of the boxplots is getting smaller. The side by side boxplots has done a good job of accounting for years but we may need to use an animation to explore all years. The crowded axes is difficult. We may want to sample countries from each quartile to reduce the number of countries on each axes. Another approach is to offset the labels slightly but this has a small chance of making a difference. 2.4.2.13 Parallel Coordinates Plot Another type of plot for our data is a parallel coordinates plot. This plot scales the data first so that the size of the variables will not dominate the plot. We will also use the data with density and population on a logarithmic scale. The order of the variables is key as well. ggparcoord(gapminder_va_log %&gt;% filter(year==1950 | year==2015),columns=4:9,group=&#39;continent&#39;, order = c(6,7,5,4,8,9)) + scale_colour_brewer(palette=&quot;Dark2&quot;,name=&quot;Continent&quot;) + facet_wrap(~year) + labs(x=&quot;Variables&quot;,y=&quot;Scaled Values&quot;) Figure 2.14: Parallel Coordinate Plot for 1950 and 2015. Population and Density are on a Log Scale. 2.4.2.14 Discussion Ideas Using Figure 2.14 answer the following questions: Do population and population density aid? What do we observe over the two time periods? What order would you use for the variables and why? We put fertility and mortality first since Europe was lower on both of the metrics in 1950 than the other continents. We place population and population density at the end since there was no discernible pattern. We again see that over time that the countries are becoming closer together in the metrics. The exception appears to be that Europe has a high percentage of the population over 60 relative to the rest of the planet. We should remove population and population density to clean up the plot, see Figure 2.15. We need the ability to identify countries and to link the plots. ggparcoord(gapminder_va_log %&gt;% filter(year==1950 | year==2015),columns=4:7,group=&#39;continent&#39;, order = c(6,7,5,4)) + scale_colour_brewer(palette=&quot;Dark2&quot;,name=&quot;Continent&quot;) + facet_wrap(~year) + labs(x=&quot;Variables&quot;,y=&quot;Scaled Values&quot;) Figure 2.15: Parallel Coordinate Plot for 1950 and 2015. Population and Density are removed. 2.4.2.15 Time Series Next we will plot year on the horizontal axis and one of the other variables of interest on the vertical axis. Figure 2.16 is an example. This has a different line for each country and as a result contains too much information. To help with this problem, could just summarize each continent instead. gapminder_va_log %&gt;% ggplot(aes(x=year,y=fertility,group=country,color=continent)) + geom_line(alpha=0.7) Figure 2.16: Fertility versus Year In Figure 2.17 we have simplified the plot by finding the median for each continent for each year. This of course assumes that the countries within a continent are similar. This is not a particularly good assumption for the Americas. gapminder_va_log %&gt;% group_by(continent,year) %&gt;% summarise(Fertility=median(fertility)) %&gt;% ggplot(aes(x=year,y=Fertility,color=continent)) + geom_line() + labs(x=&quot;Year&quot;,y=&quot;Median Fertility&quot;) Figure 2.17: Median Fertility versus Year for each Continent In Figure 2.18 we have a nice plot of the two variables fertility and life expectancy plotted next to each other. We again summarized the data at the continent level by taking the median. This erases individual country level data but we can see overall trends for each continent. gapminder_va_log %&gt;% group_by(continent,year) %&gt;% summarise(Fertility=median(fertility),&quot;Life Expectancy&quot;=median(life_exp)) %&gt;% gather(event,median,Fertility,&quot;Life Expectancy&quot;) %&gt;% ggplot(aes(x=year,y=median,color=continent)) + geom_line() + labs(x=&quot;Year&quot;,y=&quot;&quot;) + facet_wrap(~event,scales=&quot;free_y&quot;) Figure 2.18: Median Fertility and Median Life Expectancy versus Year for each Continent 2.4.2.16 Discussion Ideas What are the advantages and disadvantages of each of the plots? How can we account for country to country variation within a continent? What are some unusual insights? The time series plots have the advantage of showing the entire time history instead of a few selected time periods. However, we are limited in how to compare the different variables simultaneously. We could map variation as the line width to help understand the consistency within a continent. See Figure 2.19 for an example. There is not much difference in variance except for Europe where the variance is smaller and for the last 25 years is minimal. Around 2000, Africa had a large drop in life expectancy. In the late 1950s, Asia had a big drop in fertility. Conjectures about the cause can be made and researched. These trends would be hard to see in previous plots. gapminder_va_log %&gt;% group_by(continent,year) %&gt;% summarise(Fertility=median(fertility),Variance=sd(fertility)) %&gt;% ggplot(aes(x=year,y=Fertility,color=continent,group=continent)) + geom_line(aes(group=continent,size=Variance)) + scale_size(range=c(.1,2),guide=FALSE) + labs(x=&quot;Year&quot;,y=&quot;Median Fertility&quot;) Figure 2.19: Median Fertility with Standard Deviation as line width for each Continent 2.4.2.17 Geofacets Another approach to summarizing the data is to use geofaceting. The plots of interest are put in geographical relationship to each other. Figure ?? is an example. Here we look only at the European continent as an example, this is because the European grid map already exists. For each country we have a plot of life expectancy versus time. We show the code below but the package Bookdown was not able to render the graphic so we had export the image from R and then import into this document. # Redefine the grid to include the countries we have in our data my_eu_grid&lt;-eu_grid1[c(-6,-13,-14,-15,-16,-18,-25),] ggplot(gapminder_va_log, aes(year, life_exp)) + geom_line(color=&quot;steelblue&quot;) + facet_geo(~ country, grid = my_eu_grid) + scale_x_continuous(labels = function(x) paste0(&quot;&#39;&quot;, substr(x, 3, 4))) + ylab(&quot;Life Expectancy&quot;) + theme_bw() 2.4.2.18 Discussion Ideas What are the advantages and disadvantages of each of this plot? This plot requires the reader to know geography. It is also difficult to compare countries that are separated by distance. However, it does exhibit geographical effects along with time. It would be difficult to build a plot for the entire world. 2.4.2.19 Clustering We have been using visualization to find groupings of data. An alternative is to use hierarchical clustering. In this section we apply hierarchical clustering to our data. gap_clus &lt;- hclust(dist(gapminder_va_log %&gt;% filter(year == 1950) %&gt;% select(4:9))) plot(gap_clus,labels=(gapminder_va_log %&gt;% filter(year == 1950) %&gt;% select(continent)%&gt;% mutate(continent=as.character(continent)))[[1]], xlab=&quot;Continent&quot;) There appear to be five strong clusters. Separate out these clusters. Plot a table of continents and cluster groups. clus_res&lt;-cutree(gap_clus,k=5) table(clust_names[[2]],clus_res) ## clus_res ## 1 2 3 4 5 ## Africa 2 1 18 29 1 ## Americas 0 5 7 3 9 ## Asia 2 7 6 13 3 ## Europe 0 8 0 1 23 Table 2.9 are the countries in the second cluster. Table 2.9: Second Grouping of Countries in 1950 country continent Zimbabwe Africa Colombia Americas Costa Rica Americas Dominican Republic Americas Jamaica Americas Mexico Americas Kuwait Asia Lebanon Asia Malaysia Asia Philippines Asia Sri Lanka Asia Thailand Asia Vietnam Asia Albania Europe Bosnia and Herzegovina Europe Bulgaria Europe Montenegro Europe Poland Europe Portugal Europe Romania Europe Slovak Republic Europe Table 2.10 are the countries in the first cluster. Table 2.10: First Grouping of Contries in 1950 country continent Mali Africa Sierra Leone Africa Afghanistan Asia Yemen, Rep. Asia Next examine the year 2015. gap_clus2 &lt;- hclust(dist(gapminder_va_log %&gt;% filter(year == 2015) %&gt;% select(4:9))) plot(gap_clus2,labels=(gapminder_va_log %&gt;% filter(year == 2015) %&gt;% select(continent)%&gt;% mutate(continent=as.character(continent)))[[1]], xlab=&quot;Continent&quot;) There are now 4 distinct clusters instead of 5. clus_res2&lt;-cutree(gap_clus2,k=4) table(clust_names[[2]],clus_res2) ## clus_res2 ## 1 2 3 4 ## Africa 13 6 4 28 ## Americas 0 23 0 1 ## Asia 1 26 0 4 ## Europe 0 32 0 0 Asia, Europe, and the Americas are in a single group and Africa is split into three separate groups. Group 3 is interesting because it is only four countries. Table 2.11 contains the four countries. Table 2.11: Thrid Grouping of Contries in 2015 country continent Angola Africa Central African Republic Africa Chad Africa Somalia Africa Note that this analysis used an Euclidean distance to determine similarity between countries. Since the variables were an roughly similar scales, we did not scale the data. Although mortality and life expectancy were the largest in size. Another approach is to use correlation between countries to group. dd&lt;-as.dist(1-cor(t(gapminder_va_log %&gt;% filter(year == 1950) %&gt;% select(4:9)))) gap_clus3 &lt;- hclust(dd) plot(gap_clus3,labels=(gapminder_va_log %&gt;% filter(year == 1950) %&gt;% select(continent)%&gt;% mutate(continent=as.character(continent)))[[1]], xlab=&quot;Continent&quot;) Now there are three or four strong groups. We will use three. clus_res3&lt;-cutree(gap_clus3,k=3) table(clust_names[[2]],clus_res3) ## clus_res3 ## 1 2 3 ## Africa 50 1 0 ## Americas 15 5 4 ## Asia 28 2 1 ## Europe 8 9 15 Table 2.12 is a listing on the countries grouped together in group 3. Table 2.12: Third Grouping of Contries in 2015 country continent Canada Americas Cuba Americas United States Americas Uruguay Americas Israel Asia Australia Europe Belgium Europe Denmark Europe Finland Europe France Europe Germany Europe Greece Europe Iceland Europe Ireland Europe Netherlands Europe New Zealand Europe Norway Europe Sweden Europe Switzerland Europe United Kingdom Europe Now repeat for 2015. dd&lt;-as.dist(1-cor(t(gapminder_va_log %&gt;% filter(year == 2015) %&gt;% select(4:9)))) gap_clus4 &lt;- hclust(dd) plot(gap_clus4,labels=(gapminder_va_log %&gt;% filter(year == 2015) %&gt;% select(continent)%&gt;% mutate(continent=as.character(continent)))[[1]], xlab=&quot;Continent&quot;) Again, there are either 3 or 4 strong groups. clus_res4&lt;-cutree(gap_clus4,k=3) table(clust_names[[2]],clus_res4) ## clus_res4 ## 1 2 3 ## Africa 24 6 21 ## Americas 0 23 1 ## Asia 2 26 3 ## Europe 0 32 0 We see that Africa is split into two groups and while Asia, Europe, and the Americas are grouped together. This is the trend we have been observing in our analysis. Table 2.13 is a listing on the countries grouped together in group 2. Table 2.13: Second Grouping of Contries in 2015 country continent Algeria Africa Egypt Africa Libya Africa Mauritius Africa Morocco Africa Tunisia Africa Argentina Americas Bolivia Americas Brazil Americas Canada Americas Chile Americas Colombia Americas Costa Rica Americas Cuba Americas Dominican Republic Americas Ecuador Americas El Salvador Americas Guatemala Americas Honduras Americas Jamaica Americas Mexico Americas Nicaragua Americas Panama Americas Paraguay Americas Peru Americas Trinidad and Tobago Americas United States Americas Uruguay Americas Venezuela Americas Bahrain Asia Bangladesh Asia Cambodia Asia China Asia Indonesia Asia Iran Asia Iraq Asia Israel Asia Japan Asia Jordan Asia Korea, Dem. Rep. Asia Korea, Rep. Asia Kuwait Asia Lebanon Asia Malaysia Asia Mongolia Asia Nepal Asia Oman Asia Philippines Asia Saudi Arabia Asia Singapore Asia Sri Lanka Asia Syria Asia Thailand Asia Vietnam Asia West Bank and Gaza Asia Albania Europe Australia Europe Austria Europe Belgium Europe Bosnia and Herzegovina Europe Bulgaria Europe Croatia Europe Czech Republic Europe Denmark Europe Finland Europe France Europe Germany Europe Greece Europe Hungary Europe Iceland Europe Ireland Europe Italy Europe Montenegro Europe Netherlands Europe New Zealand Europe Norway Europe Poland Europe Portugal Europe Romania Europe Serbia Europe Slovak Republic Europe Slovenia Europe Spain Europe Sweden Europe Switzerland Europe Turkey Europe United Kingdom Europe 2.4.2.20 Discussion Ideas What are the advantages and disadvantages of clustering and listing clusters? What are some additional things you would suggest to do with clustering? The clustering does the grouping on all variables instead of just two as in previous plots. The difficulty is selecting the number of groups, the simularity measure, and the grouping metrics. The analysis above, using a correlation metric appears to anwer our original questions better. A good way to visualize the clusters is with a choropleth plot. 2.4.2.21 Choropleth Plot The code for this section was adopted from this website The following code gets the data. We have already done this and saved it in the data folder associated with this project. library(httr) # this ensures you only download the shapefile once and hides # errors and warnings. remove `try` and `invisible` to see messages try(invisible(GET(&quot;http://www.pewglobal.org/wp-content/lib/js/world-geo.json&quot;, write_disk(&quot;world-geo.json&quot;))), silent=TRUE) # use ogrListLayers(&quot;world-geo.json&quot;) to see file type &amp; # layer info to use in the call to readOGR Load the library to read the json file. library(rgdal) ## Warning: package &#39;rgdal&#39; was built under R version 3.4.4 ## Loading required package: sp ## rgdal: version: 1.3-3, (SVN revision 759) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20 ## Path to GDAL shared files: C:/Users/Brad.Warner/Documents/R/win-library/3.4/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493] ## Path to PROJ.4 shared files: C:/Users/Brad.Warner/Documents/R/win-library/3.4/rgdal/proj ## Linking to sp version: 1.3-1 Load the data and put it into a dataframe. rgdal::getGDALVersionInfo() ## [1] &quot;GDAL 2.2.3, released 2017/11/20&quot; world &lt;- readOGR(&quot;data/world-geo.json&quot;, layer=&quot;world-geo&quot;) ## OGR data source with driver: GeoJSON ## Source: &quot;C:\\Users\\Brad.Warner\\Documents\\Books\\teaching-visual-analytics\\data\\world-geo.json&quot;, layer: &quot;world-geo&quot; ## with 243 features ## It has 2 fields world_wt &lt;- spTransform(world, CRS(&quot;+proj=robin&quot;)) world_map &lt;- fortify(world_wt) ## Regions defined for each Polygons world_map %&gt;% left_join(data_frame(id=rownames(world@data), name=world@data$name)) %&gt;% select(-id) %&gt;% rename(id=name) -&gt; world_map ## Joining, by = &quot;id&quot; Check that the countries have the same names as those in the gapminder package. clust_data&lt;-cbind(clust_names,as.factor(clus_res3)) clust_data[!(clust_data[,1] %in% as.character((world_map %&gt;% distinct(id))[,1])),1] ## [1] &quot;Congo, Dem. Rep.&quot; &quot;Congo, Rep.&quot; &quot;Cote d&#39;Ivoire&quot; ## [4] &quot;Guinea-Bissau&quot; &quot;Korea, Dem. Rep.&quot; &quot;Korea, Rep.&quot; ## [7] &quot;Myanmar&quot; &quot;Slovak Republic&quot; &quot;Yemen, Rep.&quot; Save the data to a new object. my_world_map &lt;- world_map world_map_names &lt;- world_map %&gt;% distinct(id) world_map_names&lt;-levels(world_map_names[,1]) Find the name in the world map dataframe. As an example, we use Yemen. world_map_names[grep(&quot;Yemen&quot;,world_map_names)] ## [1] &quot;Yemen&quot; Fix all the names. my_world_map$id&lt;-gsub(&quot;North Korea&quot;,&quot;Korea, Dem. Rep.&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;South Korea&quot;,&quot;Korea, Rep.&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Yemen&quot;,&quot;Yemen, Rep.&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Democratic Republic of the Congo&quot;,&quot;Congo, Dem. Rep.&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Republic of the Congo&quot;,&quot;Congo, Rep.&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Burma \\\\(Myanmar\\\\)&quot;,&quot;Myanmar&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Slovakia&quot;,&quot;Slovak Republic&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Ivory Coast&quot;,&quot;Cote d&#39;Ivoire&quot;,my_world_map$id) my_world_map$id&lt;-gsub(&quot;Guinea Bissau&quot;,&quot;Guinea-Bissau&quot;,my_world_map$id) Figure 2.20 is the plot for 1950. gg &lt;-ggplot(my_world_map,aes(map_id=id)) gg &lt;- gg + geom_map(map=my_world_map,fill=&#39;white&#39;,color=&#39;black&#39;,size=0.15) +expand_limits(x=my_world_map$long,y=my_world_map$lat) gg + geom_map(map=my_world_map,data=clust_data,aes(map_id=country,fill=as.factor(clus_res3)),color=&quot;black&quot;,size=0.15) + scale_fill_manual(values=c(&quot;#fff7bc&quot;, &quot;#fec44f&quot;,&quot;#d95f0e&quot;),name=&quot;Cluster&quot;) + theme(legend.title=element_blank()) Figure 2.20: Choropleth for Clustering of Countries in 1950 And now for 2015, Figure 2.21 is the choropleth. Notice that the group numbers are arbitrary. gg &lt;-ggplot(my_world_map,aes(map_id=id)) gg &lt;- gg + geom_map(map=my_world_map,fill=&#39;white&#39;,color=&#39;black&#39;,size=0.15) +expand_limits(x=my_world_map$long,y=my_world_map$lat) gg + geom_map(map=my_world_map,data=clust_data,aes(map_id=country,fill=as.factor(clus_res4)),color=&quot;black&quot;,size=0.15) + scale_fill_manual(values=c(&quot;#fff7bc&quot;, &quot;#fec44f&quot;,&quot;#d95f0e&quot;),name=&quot;Cluster&quot;) + theme(legend.title=element_blank()) Figure 2.21: Choropleth for Clustering of Countries in 2015 2.4.3 Conclusion from Visualization From these visualizations, we can see that most of the countries in the world have over time become similar in the metrics we have selected with the expection of a large portion of Africa. In general there is an increase in life expectancy while there is a decrease in fertility. "],
["interactions-with-visualizations.html", "2.5 Interactions with Visualizations", " 2.5 Interactions with Visualizations In 2.4 we explored static visualizations of our data. This endeavor was fruitful is that we discovered some important insights. However, we did reach some limitations. Going back to 1.1, the use of interaction in our visual exploration will help overcome some of these limitations. This section implements interaction as the next step in the visual exploration. We will make use of the R implementation of the plotly javascript library, plotly (Xie 2018a). 2.5.1 First Interactive Plots We will start our exploration with the plots from Section 2.4 and data from Section 2.3. As a reminder, some of the shortcomings we found in our static visualizations were the inability to handle time, not being able to identify data points, and not link data points together. 2.5.1.1 Final Data We presented our data in Table2.6 but the table only presented the first several rows. Figure 2.22 is an interactive table, note knitr and bookdown require the use of figure instead of table in the title for items from the package DT (Xie 2018b). In this interactive table, we can change the number of rows and scroll to see the entire table. Figure 2.22: Interactive Table of Final Data 2.5.1.2 Discussion Ideas What is the life expectancy of the United States in 1984? Which country in Europe had the highest mortality in 1984? 2.5.1.3 Interactive Scatterplot Matrix We started our visual exploration of our data with a matrix scatterplot. Let’s use this same visualization, but now we will add the ability to interact with the plot. Since the plot will be too crowded with all the variables, we will drop population and density and we will remove the density plots on the diagonal and the correlation information in the upper portion of the plot. The density plots do not work well with the interaction software. gapminder_va_log %&gt;% filter(year==1950) %&gt;% ggpairs(aes(color=continent,alpha=.7,text=country), upper = list(continuous = &quot;blank&quot;, combo = &quot;box_no_facet&quot;, discrete = &quot;facetbar&quot;, na = &quot;na&quot;),diag = list(continuous = &quot;blankDiag&quot;, discrete = &quot;barDiag&quot;, na = &quot;naDiag&quot;),columns = 4:7 ) %&gt;% ggplotly(tooltip = c(&#39;continent&#39;,&#39;x&#39;,&#39;y&#39;,&#39;country&#39;)) Figure 2.23: Interactive Scatterplot Matrix for 1950 2.5.1.4 Discussion Questions Spend some time experimenting with Figure 2.23. The toolbar at the top of the plot has some options to zoom, pan, and screen capture. Also by placing the cursor over a data point, a popup window reveals information about that point. What are the countries that have low fertility and low life expectancy? Using the pan and zoom features, find the the life expectancy for the country with the largest fraction of the population above the age of 60. What is the country in the previous question? 2.5.1.5 Interactive Parallel Coordianates Plot We also used parallel coordinates plots in our visual analysis. In Figure 2.24 we have added some interactivity to this plot. ggplotly(ggparcoord(gapminder_va_log %&gt;% filter(year==1950 | year==2015),columns=c(4:7),group=&#39;continent&#39;, order = c(6,7,5,4),alphaLines = 0.5,mapping = aes(text=paste(&quot;country:&quot;, country))) + scale_colour_brewer(palette=&quot;Dark2&quot;,name=&quot;Continent&quot;) + facet_wrap(~year),tooltip = c(&#39;continent&#39;,&#39;x&#39;,&#39;text&#39;)) Figure 2.24: Interactive Scatterplot Matrix for 1950 demo(package = “plotly”) demo(“crosstalk-highlight-intro”,package = “plotly”) subplot( plot_ly(gap_1950,x=life_exp,y=fertility) %&gt;% add_markers(color=continent), plot_ly(gap_2015,x=life_exp,y=~fertility) %&gt;% add_markers(color=~continent),shareY=TRUE) %&gt;% hide_legend() p1&lt;-ggplot(gap_1950,aes(x=life_exp,y=fertility,color=continent))+geom_point() p2&lt;-ggplot(gap_2015,aes(x=life_exp,y=fertility,color=continent))+geom_point() bscols(widths = c(8, 4), d3scatter(sd_mtcars_all, ~hp, ~mpg, ~factor(cyl), x_lim = ~range(hp), y_lim = ~range(mpg), width = “100%”, height = 400), list( d3scatter(sd_mtcars_auto, ~hp, ~mpg, ~factor(cyl), x_lim = range(mtcars\\(hp), y_lim = range(mtcars\\)mpg), width = “100%”, height = 200), d3scatter(sd_mtcars_manual, ~hp, ~mpg, ~factor(cyl), x_lim = range(mtcars\\(hp), y_lim = range(mtcars\\)mpg), width = “100%”, height = 200) ) ) 2.5.1.6 Discussion Questions Spend some time experimenting with Figure 2.24. Select the continents in the legend to “turn on” and “turn off” the data. What are some advantages and disadvantages of this plot? What features should be added and which are too much? The plot are not linked. When we select a country in the right panel it is not highlighted in the left panel. One of the advantages of this plot is we have hover information although due to a limitation in the software we don’t get the country name. We can also remove the lines by select the legend names. The zooming capability is not much use in this plot and it would be a benefit to interactively reorganize the variables on the x-axis. 2.5.1.7 Linking Plots See https://rstudio.github.io/crosstalk/using.html Check d3scatter gap_1950 &lt;- SharedData$new(gapminder_va_log %&gt;% filter(year==1950),group=&quot;gap_subset&quot;) gap_2015 &lt;- SharedData$new(gapminder_va_log %&gt;% filter(year==2015),group=&quot;gap_subset&quot;) bscols(widths=c(1,5,5),filter_checkbox(&quot;continent&quot;, &quot;Continent&quot;, gap_1950, ~continent), d3scatter(gap_1950, ~life_exp, ~fertility, ~continent,width = &quot;100%&quot;), d3scatter(gap_2015, ~life_exp, ~fertility, ~continent,width = &quot;100%&quot;) ) Continent Europe Africa Americas Asia Use rbokeh rid_plot https://bokeh.pydata.org/en/latest/docs/user_guide/interaction/linking.html sd &lt;- SharedData$new(gapminder_va_log %&gt;% filter(year==1950 | year==2015) %&gt;% mutate(country=as.character(country)), ~country) p &lt;- plot_ly(sd) ggplotly(p,tooltip=c(&quot;x&quot;,&quot;country&quot;)) ## Warning: No trace type specified and no positional attributes specified ## No trace type specified: ## Based on info supplied, a &#39;scatter&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plot.ly/r/reference/#scatter ## No scatter mode specifed: ## Setting the mode to markers ## Read more about this attribute -&gt; https://plot.ly/r/reference/#scatter-mode sd &lt;- SharedData$new(gapminder_va_log %&gt;% filter(year==1950 | year==2015) %&gt;% mutate(country=as.character(country)), ~country) p &lt;- ggplot(sd, aes(life_exp, fertility,color=continent,text=country,group=year)) + geom_point() + facet_wrap(~ year) ggplotly(p,tooltip=c(&quot;x&quot;,&quot;country&quot;)) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` Use trelliscope package ggplotly(ggplot(gapminder_va %&gt;% filter(year==1982 |year==2015 | year==1950),aes(x=life_exp,y=country,color=factor(year),size=density)) + geom_point() + scale_size(name=“Density”, breaks=c(10,100,1000,4000,6000),range=c(1,7)) + facet_grid(continent~.) + theme_bw()) ggplotly(ggplot(gapminder_va_log %&gt;% filter(year==1982 |year==2015 | year==1950),aes(y=above_60,x=fertility,color=continent,text = paste(“country:”, country))) + geom_point(alpha=.7)+ facet_wrap(~year) + theme_bw()) subplot(ggplotly(ggplot(gapminder_va_log %&gt;% filter(year==1950),aes(y=above_60,x=fertility,color=continent,text = paste(“country:”, country))) + geom_point(alpha=.7),tooltip=“text”),ggplotly(ggplot(gapminder_va_log %&gt;% filter(year==2015),aes(y=above_60,x=fertility,color=continent,text = paste(“country:”, country))) + geom_point(alpha=.7),tooltip=c(‘continent’,‘x’,‘y’,‘text’))) ggplotly(ggplot(gapminder_va_log %&gt;% filter(year==1982 |year==2015 | year==1950),aes(y=above_60,x=fertility,color=continent,text = paste(“country:”, country))) + geom_point(alpha=.7)+ facet_wrap(~year) + theme_bw(),tooltip=“text”) ggplotly(ggplot(gapminder_va_log %&gt;% filter(year==1982 |year==2015 | year==1950),aes(y=above_60,x=fertility,color=continent,text = country)) + geom_point(alpha=.7)+ facet_wrap(~year) + theme_bw(),tooltip=c(“text”,“fertility”,“above_60”)) library(rbokeh) tools &lt;- c(“pan”, “wheel_zoom”, “box_zoom”, “box_select”, “reset”) nms &lt;- expand.grid(names(iris)[1:4], rev(names(iris)[1:4]), stringsAsFactors = FALSE) splom_list &lt;- vector(“list”, 16) for(ii in seq_len(nrow(nms))) { splom_list[[ii]] &lt;- figure(width = 200, height = 200, tools = tools, xlab = nms\\(Var1[ii], ylab = nms\\)Var2[ii]) %&gt;% ly_points(nms\\(Var1[ii], nms\\)Var2[ii], data = iris, color = Species, size = 5, legend = FALSE,hover = c(nms\\(Var1[ii], nms\\)Var2[ii])) } grid_plot(splom_list, ncol = 4, same_axes = TRUE, link_data = TRUE) 2.5.2 nest gapminder data by country by_country &lt;- gapminder %&gt;% group_by(country, continent) %&gt;% nest() 2.5.3 add in a plot column with map_plot by_country &lt;- by_country %&gt;% mutate( panel = map_plot(data, ~ figure(xlim = c(1948, 2011), ylim = c(10, 95), width = 300, tools = NULL) %&gt;% ly_points(year, lifeExp, data = .x, hover = .x) )) 2.5.4 plot it by_country %&gt;% trelliscope(“gapminder”, nrow = 2, ncol = 7) 2.5.5 DT options https://rstudio.github.io/DT/options.html 2.5.5.1 Animations From https://plotly-book.cpsievert.me/key-frame-animations.html base &lt;- gapminder %&gt;% plot_ly(x = ~gdpPercap, y = ~lifeExp, size = ~pop, text = ~country, hoverinfo = &quot;text&quot;) %&gt;% layout(xaxis = list(type = &quot;log&quot;)) base %&gt;% add_markers(color = ~continent, frame = ~year, ids = ~country) %&gt;% animation_opts(1000, easing = &quot;elastic&quot;, redraw = FALSE) %&gt;% animation_button( x = 1, xanchor = &quot;right&quot;, y = 0, yanchor = &quot;bottom&quot; ) %&gt;% animation_slider( currentvalue = list(prefix = &quot;YEAR &quot;, font = list(color=&quot;red&quot;)) ) meanLife &lt;- with(gapminder, tapply(lifeExp, INDEX = continent, mean)) gapminder$continent &lt;- factor( gapminder$continent, levels = names(sort(meanLife)) ) base %&gt;% add_markers(data = gapminder, frame = ~continent) %&gt;% hide_legend() %&gt;% animation_opts(frame = 1000, transition = 0, redraw = FALSE) Choropleth map in plotly https://plot.ly/r/choropleth-maps/ References "],
["Chpt3.html", "Chapter 3 Inference in Visual Analytics", " Chapter 3 Inference in Visual Analytics In Chapter 1 we introduced an visual analysis. The problem was purposefully simple and easy to understand. However, it did not offer the ability to perform more in depth analysis and inference. In this chapter we will use a similar problem to the one introduce in Chapter 1 except that it has more variables and thus the opportunity for a deeper analysis. We will not be as complete in our instruction as Chapter 1; we leave it up to the instructor to piece ideas from Chapter 1 with this work. Thus this chapter will appear to be a loose collections of ideas. "],
["lesson-preparation-differences.html", "3.1 Lesson Preparation Differences", " 3.1 Lesson Preparation Differences The lesson objectives will be similar to my_table&lt;-matrix(Male=c(3738,4704),Female=c(1494,2827)) my_table&lt;-table(my_table) row.names(my_table)&lt;-c(&quot;Admitted&quot;,&quot;Rejected&quot;) round(prop.table(my_table),3) round(prop.table(my_table,2),3) apply(my_table,1,sum) sum(my_table) (accept&lt;-apply(my_table,1,sum)[1]/sum(my_table)) apply(my_table,2,sum) apply(my_table,2,sum)*accept my_table[1,]-apply(my_table,2,sum)*accept students&lt;-c(rep(1,8442),rep(0,4321)) stat&lt;-rep(0,100000) set.seed(418) for (i in 1:100000){ temp&lt;-sample(students,5232) stat[i]&lt;-sum(temp)-3460.671 } sum(stat&gt;277.329) str(UCBAdmissions) head(UCBAdmissions) (UCBDatable&lt;-apply(UCBAdmissions, c(1, 2), sum)) round(prop.table(UCBDatable),3) round(prop.table(UCBDatable,2),3) sum(UCBDatable) (accept2&lt;-apply(UCBDatable,1,sum)[1]/sum(UCBDatable)) apply(UCBDatable,2,sum) apply(UCBDatable,2,sum)*accept2 UCBDatable[1,]-apply(UCBDatable,2,sum)*accept2 students&lt;-c(rep(1,2691),rep(0,1835)) stat&lt;-rep(0,100000) set.seed(418) for (i in 1:100000){ temp&lt;-sample(students,1755) stat[i]&lt;-sum(temp)-1043.4611 } sum(stat&gt;154.5389)/100000 library(vcd) prop.table(UCBAdmissions,c(2,3)) apply(UCBAdmissions, c(2, 3), sum) chisq.test(apply(UCBAdmissions, c(2, 3), sum)) chisq.test(UCBAdmissions) apply(UCBAdmissions, c(2, 3), sum)[1,]-(apply(UCBAdmissions,3, sum)/4526)*2691 prop.table(apply(UCBAdmissions, c(2, 3), sum),2) chisq.test(apply(UCBAdmissions, c(2, 3), sum)) round(prop.table(apply(UCBAdmissions, c(1, 3), sum),2),3) UCB &lt;- aperm(UCBAdmissions, c(2, 1, 3)) dimnames(UCB)[[2]] &lt;- c(&quot;Yes&quot;, &quot;No&quot;) names(dimnames(UCB)) &lt;- c(&quot;Sex&quot;, &quot;Admit?&quot;, &quot;Department&quot;) ftable(UCB) round(ftable(prop.table(UCB,c(1,3))),3) ftable(prop.table(UCBAdmissions,c(2,3))) ## Code for three-way table # Get data from table in frequency form UCB.data&lt;-as.data.frame(UCBAdmissions) head(UCB.data) # Since mutual independence implies marginal independence a lack of marginal independence implies a lack of mutuall independence. ##Tests for partial tables AdmitxGender for each level of Dept. oddsratio(UCBAdmissions) chisq.test(UCBAdmissions[,,1]) exp(oddsratio(UCBAdmissions[,,1])) chisq.test(UCBAdmissions[,,2]) exp(oddsratio(UCBAdmissions[,,2])) chisq.test(UCBAdmissions[,,3]) exp(oddsratio(UCBAdmissions[,,3])) chisq.test(UCBAdmissions[,,4]) exp(oddsratio(UCBAdmissions[,,4])) chisq.test(UCBAdmissions[,,5]) exp(oddsratio(UCBAdmissions[,,5])) chisq.test(UCBAdmissions[,,6]) exp(oddsratio(UCBAdmissions[,,6])) # Visualization fourfoldplot(UCBAdmissions) cotabplot(UCBAdmissions, panel = cotab_fourfold) plot(oddsratio(UCBAdmissions,log=FALSE), xlab=&quot;Department&quot;, ylab=&quot;Odds Ratio (Admit | Gender)&quot;) mosaic(UCBAdmissions,gp=shading_Friendly) #mosaic1.png mosaic(apply(UCBAdmissions, c(1, 2), sum),gp=shading_Friendly) #mosaic2.png vnames &lt;- list(set_varnames = c(Admit=&quot;Admission&quot;, Gender=&quot;Sex&quot;, Dept=&quot;Department&quot;)) lnames &lt;- list(Admit = c(&quot;Yes&quot;, &quot;No&quot;), Gender = c(&quot;Males&quot;, &quot;Females&quot;), Dept = LETTERS[1:6]) mosaic(UCBAdmissions, labeling_args=vnames, set_labels=lnames) # Mantel-Haenszel Test mantelhaen.test(UCBAdmissions) ############################# ### Berkeley admissions data for log-linear models ### Lessons 4 &amp; 5 ### See also berkeley.R in Lesson 4 ############################# ### Dataset already exist in R library UCBAdmissions # There is a oddsratio built into vcd oddsratio&lt;-function(x) x[1,1]*x[2,2]/(x[2,1]*x[1,2]) ### To test the odds-ratios in the marginal table and each of the subtables library(vcd) ### Two ways of fitting a log-linear model of complete independence ### Via loglin() function berk.ind&lt;-loglin(UCBAdmissions, list(1,2,3), fit=TRUE, param=TRUE) berk.ind ############################# ### Berkeley admissions data ### Lessons 4 &amp; 5 ### Uses dataset already in R ### See also berkeley1.R in Lesson 4 for a different code ### See also related berkeleyLoglin.R in Lesson 5 ############################# ### Dataset already exist in R library UCBAdmissions ### To test the odds-ratios in the marginal table and each of the subtables library(vcd) ##marginal table Admit x Gender admit.gender=margin.table(UCBAdmissions, c(1,2)) admit.gender admit.gender/4526 exp(oddsratio(admit.gender)) chisq.test(admit.gender) ##Tests for partial tables AdmitxGender for each level of Dept. chisq.test(UCBAdmissions[,,1]) exp(oddsratio(UCBAdmissions[,,1])) chisq.test(UCBAdmissions[,,2]) exp(oddsratio(UCBAdmissions[,,2])) chisq.test(UCBAdmissions[,,3]) exp(oddsratio(UCBAdmissions[,,3])) chisq.test(UCBAdmissions[,,4]) exp(oddsratio(UCBAdmissions[,,4])) chisq.test(UCBAdmissions[,,5]) exp(oddsratio(UCBAdmissions[,,5])) chisq.test(UCBAdmissions[,,6]) exp(oddsratio(UCBAdmissions[,,6])) ### To visualize graphically these association explore fourfold() function in the vcd() package! ### CMH test mantelhaen.test(UCBAdmissions) ### Via glm() function ####### berk.data&lt;-as.data.frame(UCBAdmissions) berk.data berk.ind&lt;-glm(berk.data$Freq~berk.data$Admit+berk.data$Gender+berk.data$Dept, family=poisson()) summary(berk.ind) fits&lt;-fitted(berk.ind) resids &lt;- residuals(berk.ind,type=&quot;pearson&quot;) h &lt;- lm.influence(berk.ind)$hat adjresids &lt;- resids/sqrt(1-h) round(cbind(berk.data$Freq,fits,adjresids),2) ### Via loglin() function berk.ind&lt;-loglin(UCBAdmissions, list(1,2,3), fit=TRUE, param=TRUE) berk.ind ##### Saturated log-linear model ### via loglin() berk.sat&lt;-loglin(UCBAdmissions, list(c(1,2,3)), fit=TRUE, param=TRUE) berk.sat ### via glm() berk.sat&lt;-glm(berk.data$Freq~berk.data$Admit*berk.data$Gender*berk.data$Dept, family=poisson()) summary(berk.sat) fitted(berk.sat) "],
["Chpt4.html", "Chapter 4 Forecasting and Prediction", " Chapter 4 Forecasting and Prediction http://junkcharts.typepad.com/junk_charts/junk-charts-trifecta-checkup-the-definitive-guide.html Use the unvotes package or NHANES data "],
["lesson-preparation-2.html", "4.1 Lesson Preparation", " 4.1 Lesson Preparation 4.1.1 Objectives: After the training, students will: explain the basic principles of visual displays of data be able to define visualization be able to use metrics to evaluate and compare visual displays of data be able 4.1.2 Preparation The following sites can be used as preparation material for this lesson, note they are arranged in a rough order of time commitment: 1) https://moz.com/blog/data-visualization-principles-lessons-from-tufte with a focus on the contents of good graphics. 2) https://www.columnfivemedia.com/makes-visualization-memorable this site offers some support of the previous link but also some counterpoints. See the paper at http://cvcl.mit.edu/papers/Borkin_etal_MemorableVisualization_TVCG2013.pdf for more details. 3) Read https://eagereyes.org/criticism/definition-of-visualization 4) Watch https://www.youtube.com/watch?v=AdSZJzb-aX8 The Art of Data Visualization 5) Watch https://www.youtube.com/watch?v=5Zg-C8AAIGg The Beauty of Data Visualization 6) Bring Data Viz into Class on Lesson One https://escholarship.org/uc/item/84v3774z For more detailed lesson or more experienced students: https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen?language=en Read Visualizing Categorical Data https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=6&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiwyPnessrUAhUL5WMKHaFhAioQFghSMAU&amp;url=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D517003&amp;usg=AFQjCNGvmx1AHiGmRZ5vHn7IPJWdStlZaQ&amp;sig2=G5Dly2rSS7I0v64zpo2T1Q Watch https://www.youtube.com/watch?v=R-oiKt7bUU8 Designing Data Visualizations with Noah Iliinsky Advances in Visualizing Categorical Data https://www.youtube.com/watch?v=qfNsoc7Tf60 Read http://neomam.com/interactive/13reasons/ Watch https://www.youtube.com/watch?v=8tJ4nBIU3q0 From the cognitive class on IBM Dark Horse Analytics three key points Less is more effective, less is more attractive, less is more impactful. They use a 3D pie as an example. https://courses.cognitiveclass.ai/courses/course-v1:CognitiveClass+DV0101EN+v1/courseware/407a9f86565c44189740699636b4fb85/12eab34ec218468995e4d06566ef4a32/ Also check out www.darkhorseanalytics.com In training session: Discuss the following: Definition from the above reading: Based on (non-visual) data. A visualization’s purpose is the communication of data. That means that the data must come from something that is abstract or at least not immediately visible (like the inside of the human body). This rules out photography and image processing. Visualization transforms from the invisible to the visible. Produce an image. It may seem obvious that a visualization has to produce an image, but that is not always so clear. Also, the visual must be the primary means of communication, other modalities can only provide additional information. If the image is only a small part of the process, it is not visualization. The result must be readable and recognizable. The most important criteria is that the visualization must provide a way to learn something about the data. Any transformation of non-trivial data into an image will leave out information, but there must be at least some relevant aspects of the data that can be read. The visualization must also be recognizable as one and not pretend to be something else (see the discussion of Informative Art). There are four important ingredients for information visualization: library(ggthemes) library(dplyr) library(htmlwidgets) library(rpivotTable) library(crosstalk) library(DT) library(leaflet) library(plotly) DT::datatable(iris) library(ggplot2) library(plotly) p &lt;- ggplot(data = diamonds, aes(x = cut, fill = clarity)) + geom_bar(position = &quot;dodge&quot;) ggplotly(p) ## We recommend that you use the dev version of ggplot2 with `ggplotly()` ## Install it with: `devtools::install_github(&#39;hadley/ggplot2&#39;)` data(mtcars) rpivotTable(mtcars,rows=&quot;gear&quot;, cols=c(&quot;cyl&quot;,&quot;carb&quot;),width=&quot;100%&quot;) devtools::install_github(&quot;jcheng5/d3scatter&quot;) library(htmlwidgets) library(crosstalk) library(leaflet) library(DT) library(d3scatter) shared_mtcars &lt;- SharedData$new(mtcars) bscols(widths = c(3,NA,NA), list( filter_checkbox(&quot;cyl&quot;, &quot;Cylinders&quot;, shared_mtcars, ~cyl, inline = TRUE), filter_slider(&quot;hp&quot;, &quot;Horsepower&quot;, shared_mtcars, ~hp, width = &quot;100%&quot;), filter_select(&quot;auto&quot;, &quot;Automatic&quot;, shared_mtcars, ~ifelse(am == 0, &quot;Yes&quot;, &quot;No&quot;)) ), d3scatter(shared_mtcars, ~wt, ~mpg, ~factor(cyl), width=&quot;100%&quot;, height=250), d3scatter(shared_mtcars, ~hp, ~qsec, ~factor(cyl), width=&quot;100%&quot;, height=250) ) Cylinders 4 6 8 Horsepower Automatic plot_ly(z = ~volcano) ## No trace type specified: ## Based on info supplied, a &#39;heatmap&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plot.ly/r/reference/#heatmap "],
["case-study.html", "Chapter 5 Case Study", " Chapter 5 Case Study Kiel’s Case Study Objectives: Students will: Appreciate the need for analytic reasoning related to data transformation Consider principles of visualization Explore the value of interactive visualizations Aircraft allocation across battlespace to provide adequate close air support coverage Background You just arrived at your new deployed job in the Strategy Division Operations Assessment Team at the Combined Air Operation Center (CAOC). The Operations Group mission planners have contacted your shop with a question about whether or not additional Close Air Support (CAS) F15E aircraft are needed, and if so, how many and at which airfield(s) they should be bedded to support which region(s) in Afghanistan. Additional questions were raised concerning your team’s recommendation for how many F-15E aircraft should be dedicated to each of the five regions of the country along with a recommended flying schedule for each region based on recent data. At the moment a fighter squadron is bedded at Airfield 1 and supports Region 1 and Region 4 using 16 F-15E aircraft. Another fighter squadron is bedded at the Airfield 2 and supports Regions 2, 3, and 5 using 24 F-15E aircraft. Each F-15E sortie departs and returns at the same time each day and consists of two F-15E aircraft providing six hours of CAS support. Additionally, each of the five regions must have 24/7 CAS coverage and each F-15E sortie can only cover one region during a single mission. Sustained 24/7 coverage over a region can therefore be provided by 8 F-15E aircraft as long as the alert aircraft (those scheduled to depart for the next sortie) are not called into action supporting the current sortie more than 25% of the time. Air refueling requirements are being handled by another member of your team and are not to be considered for the purposes of this analysis. Close Air Support (CAS) in Afghanistan largely centers on supporting Troops in Contact (TIC) events. Whenever ground forces are attacked and want air support, they declare a TIC and aircraft are dispatched to support them. For the purposes of this analysis assume each TIC is supported by a sortie of two F-15Es. Due to transit time within a region, no more than two simultaneous TICs per can be reasonably handled by an F-15E sortie, and every TIC needs to be supported by F-15E aircraft either already airborne within each region or by the two (or more) F-15E aircraft sitting alert. Airborne Warning and Control System (AWACS) serves to help coordinate CAS support to all TICs, especially those TICs that go “kinetic” (when supporting aircraft drop ordinance on targets). Your boss needs to make a recommendation as to whether more air assets are needed for current operations. We start with 200 recent close air support (CAS) events. Each row of data represents a TIC event with a date, start/stop time, location (region), and whether munitions were employed. Using this data, how do you present analysis regarding the deployment of air assets to cover close air support? "],
["AppA.html", "A Lesson Handouts", " A Lesson Handouts "],
["AppB.html", "B Project", " B Project B.0.1 Introduction "],
["references.html", "References", " References "]
]
